{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Array': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'Field': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'Map': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'SdkDataType': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'StrictFrozenModel': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'Struct': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'TectonValidationError': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BatchTriggerType(enum.Enum):\n",
      "    SCHEDULED = feature_view__args_pb2.BatchTriggerType.BATCH_TRIGGER_TYPE_SCHEDULED\n",
      "    MANUAL = feature_view__args_pb2.BatchTriggerType.BATCH_TRIGGER_TYPE_MANUAL\n",
      "    NO_BATCH_MATERIALIZATION = feature_view__args_pb2.BatchTriggerType.BATCH_TRIGGER_TYPE_NO_BATCH_MATERIALIZATION\n",
      "\n",
      "class DatabricksClusterConfig(StrictModel):\n",
      "    \"\"\"Configuration used to specify materialization cluster options on Databricks.\n",
      "\n",
      "    This class describes the attributes of the new clusters which are created in Databricks during\n",
      "    materialization jobs. You can configure options of these clusters, like cluster size and extra pip dependencies.\n",
      "\n",
      "    Note on `extra_pip_dependencies`: This is a list of packages that will be installed during materialization.\n",
      "    To use PyPI packages, specify the package name and optionally the version, e.g. `\"tensorflow\"` or `\"tensorflow==2.2.0\"`.\n",
      "    To use custom code, package it as a Python wheel or egg file in S3 or DBFS, then specify the path to the file,\n",
      "    e.g. `\"s3://my-bucket/path/custom.whl\"`, or `\"dbfs:/path/to/custom.whl\"`.\n",
      "\n",
      "    These libraries will only be available to use inside Spark UDFs. For example, if you set\n",
      "    `extra_pip_dependencies=[\"tensorflow\"]`, you can use it in your transformation as shown below.\n",
      "\n",
      "    ```python\n",
      "\n",
      "    from tecton import batch_feature_view, Input, DatabricksClusterConfig\n",
      "\n",
      "    @batch_feature_view(\n",
      "        sources=[FilteredSource(credit_scores_batch)],\n",
      "        # Can be an argument instance to a batch feature view decorator\n",
      "        batch_compute = DatabricksClusterConfig(\n",
      "            instance_type = 'm5.2xlarge',\n",
      "            spark_config = {\"spark.executor.memory\" : \"12g\"}\n",
      "            extra_pip_dependencies=[\"tensorflow\"],\n",
      "        ),\n",
      "        # Other named arguments to batch feature view\n",
      "        ...\n",
      "    )\n",
      "\n",
      "    # Use the tensorflow package in the UDF since tensorflow will be installed\n",
      "    # on the Databricks Spark cluster. The import has to be within the UDF body. Putting it at the\n",
      "    # top of the file or inside transformation function won't work.\n",
      "\n",
      "    @transformation(mode='pyspark')\n",
      "    def test_transformation(transformation_input):\n",
      "        from pyspark.sql import functions as F\n",
      "        from pyspark.sql.types import IntegerType\n",
      "\n",
      "        def my_tensorflow(x):\n",
      "            import tensorflow as tf\n",
      "            return int(tf.math.log1p(float(x)).numpy())\n",
      "\n",
      "        my_tensorflow_udf = F.udf(my_tensorflow, IntegerType())\n",
      "\n",
      "        return transformation_input.select(\n",
      "            'entity_id',\n",
      "            'timestamp',\n",
      "            my_tensorflow_udf('clicks').alias('log1p_clicks')\n",
      "        )\n",
      "    ```\n",
      "\n",
      "    :param instance_type: Instance type for the cluster. Must be a valid type as listed in [this Databricks documentation](https://databricks.com/product/aws-pricing/instance-types).\n",
      "        Additionally, Graviton instances such as the m6g family are not supported. If not specified, a value determined by the Tecton backend is used.\n",
      "    :param instance_availability: Instance availability for the cluster : \"spot\", \"on_demand\", or \"spot_with_fallback\". defaults to `spot`.\n",
      "    :param first_on_demand: The first `first_on_demand` nodes of the cluster will use on_demand instances. The rest will use the type specified by instance_availability.\n",
      "        If first_on_demand >= 1, the driver node use on_demand instance type.\n",
      "    :param number_of_workers: Number of instances for the materialization job. If not specified, a value determined by the Tecton backend is used.\n",
      "        If set to 0 then jobs will be run in single-node clusters.\n",
      "    :param extra_pip_dependencies: Extra pip dependencies to be installed on the materialization cluster. Must be PyPI packages, or wheels/eggs in S3 or DBFS.\n",
      "    :param spark_config: Map of Spark configuration options and their respective values that will be passed to the\n",
      "        FeatureView materialization Spark cluster.\n",
      "    :param dbr_version: Databricks runtime version of the cluster. Supported versions can be found on [this page](https://docs.tecton.ai/docs/materializing-features/configure-spark-materialization/using-pinned-emr-and-databricks-runtime-releases#overview)\n",
      "    \"\"\"\n",
      "    kind: Literal['DatabricksClusterConfig'] = 'DatabricksClusterConfig'\n",
      "    instance_type: Optional[str] = None\n",
      "    instance_availability: Optional[str] = None\n",
      "    number_of_workers: Optional[int] = None\n",
      "    first_on_demand: Optional[int] = None\n",
      "    extra_pip_dependencies: Optional[List[str]] = None\n",
      "    spark_config: Optional[Dict[str, str]] = None\n",
      "    dbr_version: str = DEFAULT_SPARK_VERSIONS['databricks_spark_version']\n",
      "\n",
      "class EMRClusterConfig(StrictModel):\n",
      "    \"\"\"Configuration used to specify materialization cluster options on EMR.\n",
      "\n",
      "    This class describes the attributes of the new clusters which are created in EMR during\n",
      "    materialization jobs. You can configure options of these clusters, like cluster size and extra pip dependencies.\n",
      "\n",
      "    Note on `extra_pip_dependencies`: This is a list of packages that will be installed during materialization.\n",
      "    To use PyPI packages, specify the package name and optionally the version, e.g. `\"tensorflow\"` or `\"tensorflow==2.2.0\"`.\n",
      "    To use custom code, package it as a Python wheel or egg file in S3 or DBFS, then specify the path to the file,\n",
      "    e.g. `\"s3://my-bucket/path/custom.whl\"`, or `\"dbfs:/path/to/custom.whl\"`.\n",
      "\n",
      "    These libraries will only be available to use inside Spark UDFs. For example, if you set\n",
      "    `extra_pip_dependencies=[\"tensorflow\"]`, you can use it in your transformation as shown below.\n",
      "\n",
      "    ```python\n",
      "    from tecton import batch_feature_view, Input, EMRClusterConfig\n",
      "\n",
      "    @batch_feature_view(\n",
      "        sources=[FilteredSource(credit_scores_batch)],\n",
      "        # Can be an argument instance to a batch feature view decorator\n",
      "        batch_compute = EMRClusterConfig(\n",
      "            instance_type = 'm5.2xlarge',\n",
      "            number_of_workers=4,\n",
      "            extra_pip_dependencies=[\"tensorflow==2.2.0\"],\n",
      "        ),\n",
      "        # Other named arguments to batch feature view\n",
      "        ...\n",
      "    )\n",
      "\n",
      "    # Use the tensorflow package in the UDF since tensorflow will be installed\n",
      "    # on the EMR Spark cluster. The import has to be within the UDF body. Putting it at the\n",
      "    # top of the file or inside transformation function won't work.\n",
      "\n",
      "    @transformation(mode='pyspark')\n",
      "    def test_transformation(transformation_input):\n",
      "        from pyspark.sql import functions as F\n",
      "        from pyspark.sql.types import IntegerType\n",
      "\n",
      "        def my_tensorflow(x):\n",
      "            import tensorflow as tf\n",
      "            return int(tf.math.log1p(float(x)).numpy())\n",
      "\n",
      "        my_tensorflow_udf = F.udf(my_tensorflow, IntegerType())\n",
      "\n",
      "        return transformation_input.select(\n",
      "            'entity_id',\n",
      "            'timestamp',\n",
      "            my_tensorflow_udf('clicks').alias('log1p_clicks')\n",
      "        )\n",
      "    ```\n",
      "\n",
      "    :param instance_type: Instance type for the cluster. Must be a valid type as listed in [this AWS documentation](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-supported-instance-types.html).\n",
      "        Additionally, Graviton instances such as the m6g family are not supported. If not specified, a value determined by the Tecton backend is used.\n",
      "    :param instance_availability: Instance availability for the cluster : \"spot\", \"on_demand\", or \"spot_with_fallback\". defaults to `spot`.\n",
      "    :param number_of_workers: Number of instances for the materialization job. If not specified, a value determined by the Tecton backend is used\n",
      "    :param first_on_demand: The first `first_on_demand` nodes of the cluster will use on_demand instances. The rest will use the type specified by instance_availability.\n",
      "        If first_on_demand >= 1, the master node will use on_demand instance type. `first_on_demand` is recommended to be set >= 1 for cluster configs for critical streaming features.\n",
      "    :param root_volume_size_in_gb: Size of the root volume in GB per instance for the materialization job.\n",
      "        If not specified, a value determined by the Tecton backend is used.\n",
      "    :param extra_pip_dependencies: Extra pip dependencies to be installed on the materialization cluster. Must be PyPI packages, or wheels/eggs in S3 or DBFS.\n",
      "    :param spark_config: Map of Spark configuration options and their respective values that will be passed to the\n",
      "        FeatureView materialization Spark cluster.\n",
      "    :param emr_version: EMR version of the cluster. Supported EMR versions can be found on [this page](https://docs.tecton.ai/docs/materializing-features/configure-spark-materialization/using-pinned-emr-and-databricks-runtime-releases#overview).\n",
      "    :param python_version: Python version for cluster to use. Options are \"default\" and \"python_3_9_13\" (python 3.9.13). \"default\" uses the default Python version on the EMR cluster. Defaults to \"default\".\n",
      "    \"\"\"\n",
      "    kind: Literal['EMRClusterConfig'] = 'EMRClusterConfig'\n",
      "    instance_type: Optional[str] = None\n",
      "    instance_availability: Optional[str] = None\n",
      "    number_of_workers: Optional[int] = None\n",
      "    first_on_demand: Optional[int] = None\n",
      "    root_volume_size_in_gb: Optional[int] = None\n",
      "    extra_pip_dependencies: Optional[List[str]] = None\n",
      "    spark_config: Optional[Dict[str, str]] = None\n",
      "    emr_version: str = DEFAULT_SPARK_VERSIONS['emr_spark_version']\n",
      "    python_version: str = 'default'\n",
      "\n",
      "class DatabricksJsonClusterConfig(StrictModel):\n",
      "    \"\"\"Configuration used to specify materialization clusters using json on Databricks.\n",
      "\n",
      "    This class describes the attributes of the new clusters which are created in Databricks during\n",
      "    materialization jobs. Please find more details in [User Guide](https://docs.tecton.ai/docs/materializing-features/configure-spark-materialization/configuring-job-clusters-via-json#configuring-databricks-job-clusters)\n",
      "\n",
      "    :param json: A JSON string used to directly configure the cluster used in materialization.\n",
      "    \"\"\"\n",
      "    kind: Literal['DatabricksJsonClusterConfig'] = 'DatabricksJsonClusterConfig'\n",
      "    json_: Dict[str, Any] = pydantic_v1.Field(alias='json')\n",
      "    _json_str_to_dict = pydantic_v1.validator('json_', allow_reuse=True, pre=True)(_str_to_json_struct)\n",
      "\n",
      "class DataprocJsonClusterConfig(StrictModel):\n",
      "    \"\"\"Configuration used to specify materialization clusters using json on Dataproc.\n",
      "\n",
      "    This class describes the attributes of the new clusters which are created in Dataproc during\n",
      "    materialization jobs. This feature is only available for private preview.\n",
      "\n",
      "    :param json: A JSON string used to directly configure the cluster used in materialization.\n",
      "    \"\"\"\n",
      "    kind: Literal['DataprocJsonClusterConfig'] = 'DataprocJsonClusterConfig'\n",
      "    json_: Dict[str, Any] = pydantic_v1.Field(alias='json')\n",
      "    _json_str_to_dict = pydantic_v1.validator('json_', allow_reuse=True, pre=True)(_str_to_json_struct)\n",
      "\n",
      "class EMRJsonClusterConfig(StrictModel):\n",
      "    \"\"\"Configuration used to specify materialization clusters using json on EMR.\n",
      "\n",
      "    This class describes the attributes of the new clusters which are created in EMR during\n",
      "    materialization jobs. Please find more details in this [User Guide](https://docs.tecton.ai/docs/materializing-features/configure-spark-materialization/configuring-job-clusters-via-json#configuring-emr-job-clusters)\n",
      "\n",
      "    :param json: A JSON string used to directly configure the cluster used in materialization.\n",
      "\n",
      "    \"\"\"\n",
      "    kind: Literal['EMRJsonClusterConfig'] = 'EMRJsonClusterConfig'\n",
      "    json_: Dict[str, Any] = pydantic_v1.Field(alias='json')\n",
      "    _json_str_to_dict = pydantic_v1.validator('json_', allow_reuse=True, pre=True)(_str_to_json_struct)\n",
      "\n",
      "class RiftBatchConfig(StrictModel):\n",
      "    \"\"\"Configuration used to specify materialization compute options on Rift.\n",
      "\n",
      "    :param instance_type: Instance type for the materialization job. Must be a valid EC2 instance type as listed in\n",
      "        https://aws.amazon.com/ec2/instance-types/. If not specified, a value determined by the Tecton backend is used.\n",
      "    \"\"\"\n",
      "    kind: Literal['RiftBatchConfig'] = 'RiftBatchConfig'\n",
      "    instance_type: Optional[str] = None\n",
      "\n",
      "class OfflineStoreConfig(StrictModel):\n",
      "    \"\"\"Configuration options to specify how a Feature View should materialize to the Offline Store.\n",
      "\n",
      "    :param staging_table_format: The table format for the staging table. The staging table contains partially\n",
      "        transformed feature values that have not been fully aggregated.\n",
      "    :param publish_full_features: If True, Tecton will publish a full feature values to a separate table after\n",
      "        materialization jobs to the staging table have completed. Users can query these feature values directly\n",
      "        without further transformations or aggregations.\n",
      "    :param publish_start_time: If set, Tecton will publish features starting from the feature time. If not set,\n",
      "        Tecton will default to the Feature View's feature_start_time.\n",
      "    \"\"\"\n",
      "    kind: Literal['OfflineStoreConfig'] = 'OfflineStoreConfig'\n",
      "    staging_table_format: Union[DeltaConfig, ParquetConfig] = pydantic_v1.Field(default_factory=DeltaConfig, discriminator='kind')\n",
      "    publish_full_features: bool = False\n",
      "    publish_start_time: Optional[datetime.datetime] = None\n",
      "\n",
      "class ParquetConfig(StrictModel):\n",
      "    \"\"\"(Config Class) ParquetConfig Class.\n",
      "\n",
      "    This class describes the attributes of Parquet-based offline feature store storage for the feature definition.\n",
      "\n",
      "    :param subdirectory_override: This is for the location of the feature data in the offline store. By default, all feature views will be under the subdirectory <workspace_name> if this param is not specified.\n",
      "    \"\"\"\n",
      "    kind: Literal['ParquetConfig'] = 'ParquetConfig'\n",
      "    subdirectory_override: Optional[str] = None\n",
      "\n",
      "class DeltaConfig(StrictModel):\n",
      "    \"\"\"(Config Class) DeltaConfig Class.\n",
      "\n",
      "    This class describes the attributes of DeltaLake-based offline feature store storage for the feature definition.\n",
      "\n",
      "    :param time_partition_size: The size of a time partition in the DeltaLake table, specified as a datetime.timedelta. defaults to `24 hours`.\n",
      "    :param subdirectory_override: This is for the location of the feature data in the offline store. By default, all feature views will be under the subdirectory <workspace_name> if this param is not specified.\n",
      "    \"\"\"\n",
      "    kind: Literal['DeltaConfig'] = 'DeltaConfig'\n",
      "    time_partition_size: Optional[datetime.timedelta] = datetime.timedelta(hours=24)\n",
      "    subdirectory_override: Optional[str] = None\n",
      "\n",
      "class DynamoConfig(StrictModel):\n",
      "    \"\"\"(Config Class) DynamoConfig Class.\n",
      "\n",
      "    This class describes the attributes of DynamoDB based online feature store for the feature definition.\n",
      "    Users can specify online_store = DynamoConfig()\n",
      "\n",
      "    :param replica_regions: A list of AWS regions to constrain replication of the online store to.\n",
      "        If not specified, the online store will be replicated to all available regions. An empty list\n",
      "        indicates that the DynamoDB table will only exist in the primary region, and will not be\n",
      "        replicated to any other regions. Only regions that are configured for the Tecton deployment\n",
      "        are valid inputs.\n",
      "    \"\"\"\n",
      "    kind: Literal['DynamoConfig'] = 'DynamoConfig'\n",
      "    replica_regions: Optional[List[str]] = None\n",
      "\n",
      "class RedisConfig(StrictModel):\n",
      "    \"\"\"(Config Class) RedisConfig Class.\n",
      "\n",
      "    This class describes the attributes of Redis-based online feature store for the feature definition.\n",
      "    Note : Your Tecton deployment needs to be connected to Redis before you can use this configuration option. See https://docs.tecton.ai/docs/setting-up-tecton/setting-up-other-components/connecting-redis-as-an-online-store for details and please contact Tecton Support if you need assistance.\n",
      "\n",
      "    :param primary_endpoint: Primary endpoint for the Redis Cluster. This is optional and if absent, Tecton will use the default Redis Cluster configured for your deployment.\n",
      "    :param authentication_token: Authentication token for the Redis Cluster, must be provided if primary_endpoint is present.\n",
      "    \"\"\"\n",
      "    kind: Literal['RedisConfig'] = 'RedisConfig'\n",
      "    primary_endpoint: Optional[str] = None\n",
      "    authentication_token: Optional[str] = None\n",
      "\n",
      "class BigtableConfig(StrictModel):\n",
      "    \"\"\"(Config Class) BigtableConfig Class.\n",
      "\n",
      "    This class describes the attributes of Bigtable based online feature store for the feature definition.\n",
      "    Currently there are no attributes for this class.\n",
      "    Users can specify online_store = BigtableConfig()\n",
      "\n",
      "    \"\"\"\n",
      "    kind: Literal['BigtableConfig'] = 'BigtableConfig'\n",
      "\n",
      "class CacheConfig(StrictModel):\n",
      "    \"\"\"Configuration object for feature view online caching.\n",
      "\n",
      "    :param max_age_seconds: The maximum time in seconds that features from this feature view can be cached. Must be\n",
      "        between one minute (60s) and one day (86_400s).\n",
      "    \"\"\"\n",
      "    max_age_seconds: int\n",
      "\n",
      "@attrs.define(eq=False)\n",
      "class BatchFeatureView(MaterializedFeatureView):\n",
      "    \"\"\"A Tecton Batch Feature View, used for materializing features on a batch schedule from a BatchSource.\n",
      "\n",
      "    The BatchFeatureView should not be instantiated directly, the `@batch_feature_view`\n",
      "    decorator is recommended instead.\n",
      "\n",
      "    Attributes:\n",
      "        entities: The Entities for this Feature View.\n",
      "        info: A dataclass containing basic info about this Tecton Object.\n",
      "        sources: The Data Source inputs for this Feature View.\n",
      "        transformations: The Transformations used by this Feature View.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, *, name: str, description: Optional[str]=None, owner: Optional[str]=None, tags: Optional[Dict[str, str]]=None, prevent_destroy: bool=False, feature_view_function: Callable, sources: Sequence[Union[framework_data_source.BatchSource, FilteredSource]], entities: Sequence[framework_entity.Entity], mode: str, timestamp_field: str, features: Sequence[feature.Feature], aggregation_interval: Optional[datetime.timedelta]=None, aggregation_secondary_key: Optional[str]=None, online: bool=False, offline: bool=False, ttl: Optional[datetime.timedelta]=None, feature_start_time: Optional[datetime.datetime]=None, lifetime_start_time: Optional[datetime.datetime]=None, manual_trigger_backfill_end_time: Optional[datetime.datetime]=None, batch_trigger: BatchTriggerType=BatchTriggerType.SCHEDULED, batch_schedule: Optional[datetime.timedelta]=None, online_serving_index: Optional[Sequence[str]]=None, batch_compute: Optional[configs.ComputeConfigTypes]=None, offline_store: Optional[Union[configs.OfflineStoreConfig, configs.ParquetConfig, configs.DeltaConfig]]=None, online_store: Optional[configs.OnlineStoreTypes]=None, monitor_freshness: bool=False, data_quality_enabled: Optional[bool]=None, skip_default_expectations: Optional[bool]=None, expected_feature_freshness: Optional[datetime.timedelta]=None, alert_email: Optional[str]=None, max_backfill_interval: Optional[datetime.timedelta]=None, incremental_backfills: bool=False, run_transformation_validation: Optional[bool]=None, options: Optional[Dict[str, str]]=None, tecton_materialization_runtime: Optional[str]=None, cache_config: Optional[configs.CacheConfig]=None, compaction_enabled: bool=False, environment: Optional[str]=None, context_parameter_name: Optional[str]=None, secrets: Optional[Dict[str, Union[Secret, str]]]=None, resource_providers: Optional[Dict[str, resource_provider.ResourceProvider]]=None):\n",
      "        pass\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from typing import Union, get_type_hints\n",
    "from tecton_gen_ai.testing.examples.copilot import _FuncOrClass\n",
    "import enum\n",
    "\n",
    "pool = {}\n",
    "\n",
    "def get_full_name(obj):\n",
    "    return obj.__module__ + \".\" + obj.__name__\n",
    "\n",
    "def is_tecton_type(tp):\n",
    "    try:\n",
    "        tp_str = tp.__module__\n",
    "        return tp_str.startswith(\"tecton.\") or tp_str==\"tecton\"\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "def is_abdstract(cls):\n",
    "    return inspect.isabstract(cls)\n",
    "\n",
    "def find_tecton_parents(cls):\n",
    "    for parent in cls.__bases__:\n",
    "        if is_tecton_type(parent):\n",
    "            yield parent\n",
    "        \n",
    "\n",
    "\n",
    "def find_tecton_annotations(cls):\n",
    "    try:\n",
    "        if inspect.isclass(cls):\n",
    "            annotations = get_type_hints(cls.__init__)\n",
    "        elif inspect.isfunction(cls):\n",
    "            annotations = get_type_hints(cls)\n",
    "        else:\n",
    "            raise ValueError(\"cls must be a class or a function\")\n",
    "        for name, param in annotations.items():\n",
    "            if is_tecton_type(param):\n",
    "                yield param\n",
    "            # else check if param is union, check if any of the union is a tecton type\n",
    "            elif hasattr(param, \"__origin__\") and param.__origin__ == Union:\n",
    "                for arg in param.__args__:\n",
    "                    if is_tecton_type(arg):\n",
    "                        yield arg\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def find_tecton_mentions(obj):\n",
    "    # check obj.obj is a class type use inspect\n",
    "    if inspect.isclass(obj):\n",
    "        if not issubclass(obj, enum.Enum):\n",
    "            yield from find_tecton_annotations(obj)\n",
    "    elif inspect.isfunction(obj):\n",
    "        yield from find_tecton_annotations(obj)\n",
    "\n",
    "def find_tecton_dependencies(scope):\n",
    "    res = {}\n",
    "    for key, obj in scope.items():\n",
    "        res[key]= list(x.__name__ for x in find_tecton_mentions(obj.obj) if x.__name__ in scope)\n",
    "    return res\n",
    "\n",
    "def build_api_graph():\n",
    "    objects = list(_FuncOrClass.from_expressions([\"tecton\"]))\n",
    "    scope = {x.name: x for x in objects}\n",
    "    deps = find_tecton_dependencies(scope)\n",
    "    res = {}\n",
    "    for key, value in scope.items():\n",
    "        res[key] = {\"declaration\": value.callable_declaration, \"deps\": deps[key]}\n",
    "    return res\n",
    "\n",
    "def build_code(name, graph, code):\n",
    "    if name in code:\n",
    "        return\n",
    "    for dep in graph[name][\"deps\"]:\n",
    "        build_code(dep, graph, code)\n",
    "    code[name] = graph[name][\"declaration\"]\n",
    "\n",
    "\n",
    "graph = build_api_graph()\n",
    "\n",
    "code = {}\n",
    "build_code(\"BatchFeatureView\", graph, code)\n",
    "print(\"\\n\\n\".join(code.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tecton.framework.feature.Aggregate\n",
      "<class 'tecton.aggregation_functions.AggregationFunction'>\n",
      "<class 'tecton.framework.configs.TimeWindow'>\n",
      "<class 'tecton.framework.configs.TimeWindowSeries'>\n",
      "<class 'tecton.framework.configs.LifetimeWindow'>\n",
      "tecton.framework.feature_view.AggregationLeadingEdge\n",
      "tecton.framework.feature.Attribute\n",
      "tecton.framework.configs.AutoscalingConfig\n",
      "tecton.framework.feature_view.BatchFeatureView\n",
      "tecton.framework.data_source.BatchSource\n",
      "tecton.framework.feature_view.BatchTriggerType\n",
      "tecton.framework.configs.BigQueryConfig\n",
      "<class 'tecton.framework.utils.Secret'>\n",
      "tecton.framework.configs.BigtableConfig\n",
      "tecton.framework.configs.CacheConfig\n",
      "tecton.framework.feature.Calculation\n",
      "tecton_core.compute_mode.ComputeMode\n",
      "tecton.framework.data_frame.TectonDataFrame\n",
      "tecton.framework.data_source.DataSource\n",
      "tecton.framework.configs.DatabricksClusterConfig\n",
      "tecton.framework.configs.DatabricksJsonClusterConfig\n",
      "tecton.framework.configs.DataprocJsonClusterConfig\n",
      "tecton.framework.dataset.Dataset\n",
      "<class 'tecton.framework.dataset.Dataset'> 'wrapper_descriptor' object has no attribute '__annotations__'\n",
      "tecton.framework.configs.DatetimePartitionColumn\n",
      "tecton.framework.configs.DeltaConfig\n",
      "tecton.framework.configs.DynamoConfig\n",
      "tecton.framework.configs.EMRClusterConfig\n",
      "tecton.framework.configs.EMRJsonClusterConfig\n",
      "tecton.framework.feature.Embedding\n",
      "tecton.framework.entity.Entity\n",
      "tecton.framework.feature.FeatureMetadata\n",
      "tecton.framework.feature_view.FeatureReference\n",
      "tecton.framework.server_group.FeatureServerGroup\n",
      "<class 'tecton.framework.configs.AutoscalingConfig'>\n",
      "<class 'tecton.framework.configs.ProvisionedScalingConfig'>\n",
      "tecton.framework.feature_service.FeatureService\n",
      "<class 'tecton.framework.configs.LoggingConfig'>\n",
      "<class 'tecton.framework.server_group.TransformServerGroup'>\n",
      "tecton.framework.feature_view.FeatureTable\n",
      "tecton.framework.data_frame.FeatureVector\n",
      "tecton.framework.feature_view.FeatureView\n",
      "tecton.framework.configs.FileConfig\n",
      "tecton_core.filter_context.FilterContext\n",
      "tecton.framework.configs.HiveConfig\n",
      "tecton.framework.feature.Inference\n",
      "tecton.framework.configs.KafkaConfig\n",
      "tecton.framework.configs.KafkaOutputStream\n",
      "tecton.framework.configs.KinesisConfig\n",
      "tecton.framework.configs.KinesisOutputStream\n",
      "tecton.framework.configs.LifetimeWindow\n",
      "tecton.framework.configs.LoggingConfig\n",
      "tecton._internals.materialization_api.MaterializationAttempt\n",
      "tecton_core.materialization_context.MaterializationContext\n",
      "tecton._internals.materialization_api.MaterializationJob\n",
      "tecton.framework.feature_view.MaterializedFeatureView\n",
      "tecton_core.mock_context.MockContext\n",
      "tecton.framework.model_config.ModelConfig\n",
      "tecton.framework.configs.OfflineStoreConfig\n",
      "tecton_core.online_serving_index.OnlineServingIndex\n",
      "tecton.framework.configs.PandasBatchConfig\n",
      "<class 'tecton.framework.configs.PandasBatchConfig.PandasDataSourceFunctionType1'>\n",
      "<class 'tecton.framework.configs.PandasBatchConfig.PandasDataSourceFunctionType2'>\n",
      "<class 'tecton.framework.configs.PandasBatchConfig.PandasDataSourceFunctionType3'>\n",
      "<class 'tecton.framework.configs.PandasBatchConfig.PandasDataSourceFunctionType4'>\n",
      "tecton.framework.configs.ParquetConfig\n",
      "tecton.framework.configs.ProvisionedScalingConfig\n",
      "tecton.framework.configs.PushConfig\n",
      "tecton.framework.configs.PyArrowBatchConfig\n",
      "<class 'tecton.framework.configs.PyArrowBatchConfig.PyArrowDataSourceFunctionType1'>\n",
      "<class 'tecton.framework.configs.PyArrowBatchConfig.PyArrowDataSourceFunctionType2'>\n",
      "<class 'tecton.framework.configs.PyArrowBatchConfig.PyArrowDataSourceFunctionType3'>\n",
      "<class 'tecton.framework.configs.PyArrowBatchConfig.PyArrowDataSourceFunctionType4'>\n",
      "tecton_core.realtime_context.RealtimeContext\n",
      "tecton.framework.feature_view.RealtimeFeatureView\n",
      "tecton.framework.configs.RedisConfig\n",
      "tecton.framework.configs.RedshiftConfig\n",
      "tecton.framework.configs.RequestSource\n",
      "tecton.framework.configs.RiftBatchConfig\n",
      "tecton.framework.utils.Secret\n",
      "tecton.framework.configs.SnowflakeConfig\n",
      "<class 'tecton.framework.utils.Secret'>\n",
      "<class 'tecton.framework.utils.Secret'>\n",
      "tecton.framework.configs.SparkBatchConfig\n",
      "tecton.framework.configs.SparkStreamConfig\n",
      "tecton.framework.feature_view.StreamFeatureView\n",
      "tecton.framework.feature_view.StreamProcessingMode\n",
      "tecton.framework.data_source.StreamSource\n",
      "tecton.framework.data_frame.TectonDataFrame\n",
      "tecton_core.filter_utils.TectonTimeConstant\n",
      "tecton.tecton_test_repo.TestRepo\n",
      "tecton.framework.configs.TimeWindow\n",
      "tecton.framework.configs.TimeWindowSeries\n",
      "tecton.framework.server_group.TransformServerGroup\n",
      "<class 'tecton.framework.configs.AutoscalingConfig'>\n",
      "<class 'tecton.framework.configs.ProvisionedScalingConfig'>\n",
      "tecton.framework.transformation.Transformation\n",
      "tecton.framework.configs.UnityCatalogAccessMode\n",
      "tecton.framework.configs.UnityConfig\n",
      "tecton.framework.workspace.Workspace\n",
      "tecton.framework.feature_view\n",
      "tecton.identities.credentials\n",
      "tecton.framework.transformation\n",
      "tecton.identities.credentials\n",
      "tecton.tecton_context\n",
      "tecton.framework.workspace\n",
      "tecton.framework.workspace\n",
      "tecton.framework.workspace\n",
      "tecton.framework.workspace\n",
      "tecton.framework.workspace\n",
      "tecton.framework.workspace\n",
      "tecton.framework.workspace\n",
      "tecton.fco_listers\n",
      "tecton.identities.credentials\n",
      "tecton.identities.credentials\n",
      "tecton_core.materialization_context\n",
      "tecton.framework.configs\n",
      "tecton.framework.configs\n",
      "tecton.framework.feature_view\n",
      "tecton.framework.resource_provider\n",
      "tecton.tecton_context\n",
      "tecton.framework.configs\n",
      "tecton.framework.configs\n",
      "tecton.framework.feature_view\n",
      "tecton.framework.transformation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\"Aggregate\", \"AggregationFunction\", \"AggregationLeadingEdge\", \"Attribute\", \"AutoscalingConfig\", \"BatchFeatureView\", \"BatchSource\", \"BatchTriggerType\", \"BigQueryConfig\", \"BigtableConfig\", \"CacheConfig\", \"Calculation\", \"DataSource\", \"DatabricksClusterConfig\", \"DatabricksJsonClusterConfig\", \"DataprocJsonClusterConfig\", \"Dataset\", \"DatetimePartitionColumn\", \"DeltaConfig\", \"DynamoConfig\", \"EMRClusterConfig\", \"EMRJsonClusterConfig\", \"Embedding\", \"Entity\", \"FeatureMetadata\", \"FeatureReference\", \"FeatureServerGroup\", \"FeatureService\", \"FeatureTable\", \"FeatureVector\", \"FeatureView\", \"Field\", \"FileConfig\", \"HiveConfig\", \"Inference\", \"KafkaConfig\", \"KafkaOutputStream\", \"KinesisConfig\", \"KinesisOutputStream\", \"LifetimeWindow\", \"LoggingConfig\", \"MaterializationAttempt\", \"MaterializationJob\", \"MaterializedFeatureView\", \"ModelConfig\", \"OfflineStoreConfig\", \"PandasBatchConfig\", \"PandasDataSourceFunctionType1\", \"PandasDataSourceFunctionType2\", \"PandasDataSourceFunctionType3\", \"PandasDataSourceFunctionType4\", \"ParquetConfig\", \"ProvisionedScalingConfig\", \"PushConfig\", \"PyArrowBatchConfig\", \"PyArrowDataSourceFunctionType1\", \"PyArrowDataSourceFunctionType2\", \"PyArrowDataSourceFunctionType3\", \"PyArrowDataSourceFunctionType4\", \"RealtimeFeatureView\", \"RedisConfig\", \"RedshiftConfig\", \"RequestSource\", \"RiftBatchConfig\", \"SdkDataType\", \"Secret\", \"SnowflakeConfig\", \"SparkBatchConfig\", \"SparkStreamConfig\", \"StreamFeatureView\", \"StreamProcessingMode\", \"StreamSource\", \"TectonDataFrame\", \"TectonObjectInfo\", \"TestRepo\", \"TimeWindow\", \"TimeWindowSeries\", \"TransformServerGroup\", \"Transformation\", \"UnityCatalogAccessMode\", \"UnityConfig\", \"Workspace\", \"batch_feature_view\", \"complete_login\", \"const\", \"get_caller_identity\", \"get_current_workspace\", \"get_data_source\", \"get_entity\", \"get_feature_service\", \"get_feature_table\", \"get_feature_view\", \"get_transformation\", \"get_workspace\", \"list_workspaces\", \"login\", \"logout\", \"pandas_batch_config\", \"pyarrow_batch_config\", \"realtime_feature_view\", \"resource_provider\", \"set_tecton_spark_session\", \"spark_batch_config\", \"spark_stream_config\", \"stream_feature_view\", \"transformation\"]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "all_class_names = sorted(set([x.__name__ for x in find_tecton_mentions(objects)]))\n",
    "all_class_names = json.dumps(all_class_names)\n",
    "all_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.testing import set_dev_mode\n",
    "from tecton_gen_ai.api import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Output(BaseModel):\n",
    "    code: str = Field(..., description=\"The generated python code\")\n",
    "\n",
    "set_dev_mode()\n",
    "\n",
    "agent = Agent(\n",
    "    name = \"parser\",\n",
    "    prompt = \"\"\"\n",
    "You are given a declaration of a class or a function, convert it to a pydantic model representation\n",
    "You only need to output the pydantic model code, no import needed\n",
    "You need to make sure each field is defined using pydantic Field, but you should use `PField` instead\n",
    "\n",
    "parameters docstrings should not be included in the final class docstring\n",
    "\n",
    "For example\n",
    "\n",
    "```python\n",
    "class SomeView:\n",
    "    '''This is a class docstring for SomeView'''\n",
    "\n",
    "    def __init__(self, a: int, b: Entity):\n",
    "        '''\n",
    "        more explanations 1\n",
    "\n",
    "        :param a: parameter a\n",
    "        :param b: parameter b\n",
    "\n",
    "        more explanations 2\n",
    "        '''\n",
    "        pass\n",
    "```\n",
    "\n",
    "should be translated to\n",
    "\n",
    "```python\n",
    "class SomeView(BaseModel):\n",
    "    '''This is a class docstring for SomeView\n",
    "    \n",
    "    more explanations 1\n",
    "\n",
    "    more explanations 2\n",
    "    '''\n",
    "\n",
    "    a: int = PField(..., description=\"parameter a\")\n",
    "    b: Entity = PField(..., description=\"parameter b\")\n",
    "```\n",
    "\n",
    "The output should just be the python code without backticks\n",
    "\n",
    "\"\"\",\n",
    "    llm = {\"model\": \"openai/gpt-4o-2024-11-20\", \"temperature\": 0.0},\n",
    "    output_schema=Output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:28<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "code = []\n",
    "\n",
    "for obj in tqdm.tqdm(objects[:5]):\n",
    "    if inspect.isclass(obj.obj):\n",
    "        res = agent.invoke(obj.declaration)\n",
    "        code.append(res[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Aggregate(BaseModel):\n",
      "    '''The `Aggregate` class describes an aggregation feature that is applied to a Batch or Stream Feature View via `features` param.\n",
      "\n",
      "    ```python\n",
      "    from tecton import Aggregate, batch_feature_view, TimeWindow\n",
      "    from tecton.types import Int64\n",
      "    from datetime import timedelta\n",
      "\n",
      "    @batch_feature_view(\n",
      "        # ...\n",
      "        features=[\n",
      "            Aggregate(\n",
      "                input_column=Field(\"my_column\", Int64),\n",
      "                function=\"mean\",\n",
      "                time_window=TimeWindow(window_size=timedelta(days=7)),\n",
      "            ),\n",
      "            Aggregate(\n",
      "                input_column=Field(\"another_column\", Int64),\n",
      "                function=\"mean\",\n",
      "                time_window=TimeWindow(window_size=timedelta(days=1)),\n",
      "                name=\"1d_average\",\n",
      "                description=\"my aggregate feature description\",\n",
      "                tags={\"tag\": \"value\"}\n",
      "            ),\n",
      "        ],\n",
      "    )\n",
      "    def my_fv(data_source):\n",
      "        pass\n",
      "    ```\n",
      "    '''\n",
      "\n",
      "    function: AggregationFunction = PField(..., description=\"One of the built-in aggregation functions, such as 'sum', 'count', `last(2)` etc.\")\n",
      "    time_window: Union[TimeWindow, TimeWindowSeries, LifetimeWindow] = PField(..., description=\"The window_size and optional offset over which to aggregate over.\")\n",
      "    name: Optional[str] = PField(None, description=\"The name of this feature. Defaults to an autogenerated name, e.g. transaction_count_7d_1d.\")\n",
      "    input_column: Field = PField(..., description=\"Describes name and type of the column that will be used in the aggregation.\")\n",
      "    description: Optional[str] = PField(None, description=\"A human-readable description of the feature\")\n",
      "    tags: Optional[Dict[str, str]] = PField(None, description=\"Tags associated with the feature (key-value pairs of user-defined metadata).\")\n",
      "\n",
      "class AggregationLeadingEdge(BaseModel):\n",
      "    \"\"\"Defines the leading edge timestamp for aggregation windows in stream feature views during online retrieval.\n",
      "\n",
      "    WALL_CLOCK_TIME: Stream aggregation windows are fetched relative to the wall clock time at the time of online retrieval.\n",
      "    LATEST_EVENT_TIME: Stream aggregation windows are fetched relative the latest materialized event timestamp for the stream feature view. This timestamp is also known as the stream high watermark.\n",
      "\n",
      "    Example:\n",
      "        For a stream that is 30-seconds delayed and a 10-minute aggregation window:\n",
      "        - WALL_CLOCK_TIME at 12:00:00 uses the window [11:50:00, 12:00:00]\n",
      "        - LATEST_EVENT_TIME at 12:00:00 uses the window [11:49:30, 11:59:30]\n",
      "\n",
      "    Refer to documentation for detailed implications of each option.\n",
      "    \"\"\"\n",
      "\n",
      "    UNSPECIFIED: str = PField(..., description=\"Unspecified aggregation mode.\")\n",
      "    WALL_CLOCK_TIME: str = PField(..., description=\"Aggregation mode based on wall clock time.\")\n",
      "    LATEST_EVENT_TIME: str = PField(..., description=\"Aggregation mode based on the latest event time.\")\n",
      "\n",
      "class Attribute(BaseModel):\n",
      "    \"\"\"The `Attribute` class describes an attribute feature that is applied to a Feature View or Feature Table via `features` param.\n",
      "\n",
      "    ```python\n",
      "    from tecton import Attribute, batch_feature_view\n",
      "    from tecton.types import String\n",
      "\n",
      "    @batch_feature_view(\n",
      "        # ...\n",
      "        features=[\n",
      "            Attribute(\n",
      "                name=\"my_column\",\n",
      "                dtype=String\n",
      "            )\n",
      "            Attribute(\n",
      "                name=\"my_other_column\",\n",
      "                dtype=String,\n",
      "                description=\"my attribute feature description\",\n",
      "                tags={\"tag\": \"value\"}\n",
      "            )\n",
      "        ],\n",
      "    )\n",
      "    def my_fv(data_source):\n",
      "        pass\n",
      "    ```\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    name: str = PField(..., description=\"Name of a column from the transformation output.\")\n",
      "    dtype: SdkDataType = PField(..., description=\"Datatype of a column from the transformation output\")\n",
      "    description: Optional[str] = PField(None, description=\"A human-readable description of the feature\")\n",
      "    tags: Optional[Dict[str, str]] = PField(None, description=\"Tags associated with the feature (key-value pairs of user-defined metadata).\")\n",
      "\n",
      "class AutoscalingConfig(BaseModel):\n",
      "    \"\"\"Configuration for autoscaling of server groups.\"\"\"\n",
      "\n",
      "    min_nodes: Optional[int] = PField(None, description=\"The minimum number of nodes to scale down to.\")\n",
      "    max_nodes: Optional[int] = PField(None, description=\"The maximum number of nodes to scale up to.\")\n",
      "\n",
      "class BatchFeatureView(BaseModel):\n",
      "    \"\"\"A Tecton Batch Feature View, used for materializing features on a batch schedule from a BatchSource.\n",
      "\n",
      "    The BatchFeatureView should not be instantiated directly, the `@batch_feature_view`\n",
      "    decorator is recommended instead.\n",
      "\n",
      "    Attributes:\n",
      "        entities: The Entities for this Feature View.\n",
      "        info: A dataclass containing basic info about this Tecton Object.\n",
      "        sources: The Data Source inputs for this Feature View.\n",
      "        transformations: The Transformations used by this Feature View.\n",
      "    \"\"\"\n",
      "\n",
      "    name: str = PField(...)\n",
      "    description: Optional[str] = PField(None)\n",
      "    owner: Optional[str] = PField(None)\n",
      "    tags: Optional[Dict[str, str]] = PField(None)\n",
      "    prevent_destroy: bool = PField(False)\n",
      "    feature_view_function: Callable = PField(...)\n",
      "    sources: Sequence[Union[framework_data_source.BatchSource, FilteredSource]] = PField(...)\n",
      "    entities: Sequence[framework_entity.Entity] = PField(...)\n",
      "    mode: str = PField(...)\n",
      "    timestamp_field: str = PField(...)\n",
      "    features: Sequence[feature.Feature] = PField(...)\n",
      "    aggregation_interval: Optional[datetime.timedelta] = PField(None)\n",
      "    aggregation_secondary_key: Optional[str] = PField(None)\n",
      "    online: bool = PField(False)\n",
      "    offline: bool = PField(False)\n",
      "    ttl: Optional[datetime.timedelta] = PField(None)\n",
      "    feature_start_time: Optional[datetime.datetime] = PField(None)\n",
      "    lifetime_start_time: Optional[datetime.datetime] = PField(None)\n",
      "    manual_trigger_backfill_end_time: Optional[datetime.datetime] = PField(None)\n",
      "    batch_trigger: BatchTriggerType = PField(BatchTriggerType.SCHEDULED)\n",
      "    batch_schedule: Optional[datetime.timedelta] = PField(None)\n",
      "    online_serving_index: Optional[Sequence[str]] = PField(None)\n",
      "    batch_compute: Optional[configs.ComputeConfigTypes] = PField(None)\n",
      "    offline_store: Optional[Union[configs.OfflineStoreConfig, configs.ParquetConfig, configs.DeltaConfig]] = PField(None)\n",
      "    online_store: Optional[configs.OnlineStoreTypes] = PField(None)\n",
      "    monitor_freshness: bool = PField(False)\n",
      "    data_quality_enabled: Optional[bool] = PField(None)\n",
      "    skip_default_expectations: Optional[bool] = PField(None)\n",
      "    expected_feature_freshness: Optional[datetime.timedelta] = PField(None)\n",
      "    alert_email: Optional[str] = PField(None)\n",
      "    max_backfill_interval: Optional[datetime.timedelta] = PField(None)\n",
      "    incremental_backfills: bool = PField(False)\n",
      "    run_transformation_validation: Optional[bool] = PField(None)\n",
      "    options: Optional[Dict[str, str]] = PField(None)\n",
      "    tecton_materialization_runtime: Optional[str] = PField(None)\n",
      "    cache_config: Optional[configs.CacheConfig] = PField(None)\n",
      "    compaction_enabled: bool = PField(False)\n",
      "    environment: Optional[str] = PField(None)\n",
      "    context_parameter_name: Optional[str] = PField(None)\n",
      "    secrets: Optional[Dict[str, Union[Secret, str]]] = PField(None)\n",
      "    resource_providers: Optional[Dict[str, resource_provider.ResourceProvider]] = PField(None)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
