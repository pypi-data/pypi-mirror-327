import array as pyarray
import numpy as np
from _typeshed import Incomplete
from py4j.java_gateway import JavaObject
from pyspark import SparkContext
from pyspark.mllib._typing import VectorLike
from pyspark.mllib.common import JavaModelWrapper
from pyspark.mllib.stat.distribution import MultivariateGaussian
from pyspark.mllib.util import JavaLoader, JavaSaveable, Loader, Saveable
from pyspark.rdd import RDD
from pyspark.streaming import DStream
from typing import NamedTuple, TypeVar, overload

__all__ = ['BisectingKMeansModel', 'BisectingKMeans', 'KMeansModel', 'KMeans', 'GaussianMixtureModel', 'GaussianMixture', 'PowerIterationClusteringModel', 'PowerIterationClustering', 'StreamingKMeans', 'StreamingKMeansModel', 'LDA', 'LDAModel']

T = TypeVar('T')

class BisectingKMeansModel(JavaModelWrapper):
    centers: Incomplete
    def __init__(self, java_model: JavaObject) -> None: ...
    @property
    def clusterCenters(self) -> list[np.ndarray]: ...
    @property
    def k(self) -> int: ...
    @overload
    def predict(self, x: VectorLike) -> int: ...
    @overload
    def predict(self, x: RDD['VectorLike']) -> RDD[int]: ...
    def computeCost(self, x: VectorLike | RDD['VectorLike']) -> float: ...

class BisectingKMeans:
    @classmethod
    def train(cls, rdd: RDD['VectorLike'], k: int = 4, maxIterations: int = 20, minDivisibleClusterSize: float = 1.0, seed: int = -1888008604) -> BisectingKMeansModel: ...

class KMeansModel(Saveable, Loader['KMeansModel']):
    centers: Incomplete
    def __init__(self, centers: list['VectorLike']) -> None: ...
    @property
    def clusterCenters(self) -> list['VectorLike']: ...
    @property
    def k(self) -> int: ...
    @overload
    def predict(self, x: VectorLike) -> int: ...
    @overload
    def predict(self, x: RDD['VectorLike']) -> RDD[int]: ...
    def computeCost(self, rdd: RDD['VectorLike']) -> float: ...
    def save(self, sc: SparkContext, path: str) -> None: ...
    @classmethod
    def load(cls, sc: SparkContext, path: str) -> KMeansModel: ...

class KMeans:
    @classmethod
    def train(cls, rdd: RDD['VectorLike'], k: int, maxIterations: int = 100, initializationMode: str = 'k-means||', seed: int | None = None, initializationSteps: int = 2, epsilon: float = 0.0001, initialModel: KMeansModel | None = None, distanceMeasure: str = 'euclidean') -> KMeansModel: ...

class GaussianMixtureModel(JavaModelWrapper, JavaSaveable, JavaLoader['GaussianMixtureModel']):
    @property
    def weights(self) -> np.ndarray: ...
    @property
    def gaussians(self) -> list[MultivariateGaussian]: ...
    @property
    def k(self) -> int: ...
    @overload
    def predict(self, x: VectorLike) -> np.int64: ...
    @overload
    def predict(self, x: RDD['VectorLike']) -> RDD[int]: ...
    @overload
    def predictSoft(self, x: VectorLike) -> np.ndarray: ...
    @overload
    def predictSoft(self, x: RDD['VectorLike']) -> RDD[pyarray.array]: ...
    @classmethod
    def load(cls, sc: SparkContext, path: str) -> GaussianMixtureModel: ...

class GaussianMixture:
    @classmethod
    def train(cls, rdd: RDD['VectorLike'], k: int, convergenceTol: float = 0.001, maxIterations: int = 100, seed: int | None = None, initialModel: GaussianMixtureModel | None = None) -> GaussianMixtureModel: ...

class PowerIterationClusteringModel(JavaModelWrapper, JavaSaveable, JavaLoader['PowerIterationClusteringModel']):
    @property
    def k(self) -> int: ...
    def assignments(self) -> RDD['PowerIterationClustering.Assignment']: ...
    @classmethod
    def load(cls, sc: SparkContext, path: str) -> PowerIterationClusteringModel: ...

class PowerIterationClustering:
    @classmethod
    def train(cls, rdd: RDD[tuple[int, int, float]], k: int, maxIterations: int = 100, initMode: str = 'random') -> PowerIterationClusteringModel: ...
    class Assignment(NamedTuple('Assignment', [('id', Incomplete), ('cluster', Incomplete)])): ...

class StreamingKMeansModel(KMeansModel):
    def __init__(self, clusterCenters: list['VectorLike'], clusterWeights: VectorLike) -> None: ...
    @property
    def clusterWeights(self) -> list[np.float64]: ...
    centers: Incomplete
    def update(self, data: RDD['VectorLike'], decayFactor: float, timeUnit: str) -> StreamingKMeansModel: ...

class StreamingKMeans:
    def __init__(self, k: int = 2, decayFactor: float = 1.0, timeUnit: str = 'batches') -> None: ...
    def latestModel(self) -> StreamingKMeansModel | None: ...
    def setK(self, k: int) -> StreamingKMeans: ...
    def setDecayFactor(self, decayFactor: float) -> StreamingKMeans: ...
    def setHalfLife(self, halfLife: float, timeUnit: str) -> StreamingKMeans: ...
    def setInitialCenters(self, centers: list['VectorLike'], weights: list[float]) -> StreamingKMeans: ...
    def setRandomCenters(self, dim: int, weight: float, seed: int) -> StreamingKMeans: ...
    def trainOn(self, dstream: DStream[VectorLike]) -> None: ...
    def predictOn(self, dstream: DStream[VectorLike]) -> DStream[int]: ...
    def predictOnValues(self, dstream: DStream[tuple[T, VectorLike]]) -> DStream[tuple[T, int]]: ...

class LDAModel(JavaModelWrapper, JavaSaveable, Loader['LDAModel']):
    def topicsMatrix(self) -> np.ndarray: ...
    def vocabSize(self) -> int: ...
    def describeTopics(self, maxTermsPerTopic: int | None = None) -> list[tuple[list[int], list[float]]]: ...
    @classmethod
    def load(cls, sc: SparkContext, path: str) -> LDAModel: ...

class LDA:
    @classmethod
    def train(cls, rdd: RDD[tuple[int, 'VectorLike']], k: int = 10, maxIterations: int = 20, docConcentration: float = -1.0, topicConcentration: float = -1.0, seed: int | None = None, checkpointInterval: int = 10, optimizer: str = 'em') -> LDAModel: ...
