from datetime import datetime
from py4j.java_gateway import JavaObject
from pyspark.rdd import RDD
from pyspark.resultiterable import ResultIterable
from pyspark.serializers import Serializer
from pyspark.storagelevel import StorageLevel
from pyspark.streaming.context import StreamingContext
from typing import Callable, Generic, Hashable, Iterable, TypeVar, overload

__all__ = ['DStream']

S = TypeVar('S')
T = TypeVar('T')
T_co = TypeVar('T_co', covariant=True)
U = TypeVar('U')
K = TypeVar('K', bound=Hashable)
V = TypeVar('V')

class DStream(Generic[T_co]):
    is_cached: bool
    is_checkpointed: bool
    def __init__(self, jdstream: JavaObject, ssc: StreamingContext, jrdd_deserializer: Serializer) -> None: ...
    def context(self) -> StreamingContext: ...
    def count(self) -> DStream[int]: ...
    def filter(self, f: Callable[[T], bool]) -> DStream[T]: ...
    def flatMap(self, f: Callable[[T], Iterable[U]], preservesPartitioning: bool = False) -> DStream[U]: ...
    def map(self, f: Callable[[T], U], preservesPartitioning: bool = False) -> DStream[U]: ...
    def mapPartitions(self, f: Callable[[Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> DStream[U]: ...
    def mapPartitionsWithIndex(self, f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> DStream[U]: ...
    def reduce(self, func: Callable[[T, T], T]) -> DStream[T]: ...
    def reduceByKey(self, func: Callable[[V, V], V], numPartitions: int | None = None) -> DStream[tuple[K, V]]: ...
    def combineByKey(self, createCombiner: Callable[[V], U], mergeValue: Callable[[U, V], U], mergeCombiners: Callable[[U, U], U], numPartitions: int | None = None) -> DStream[tuple[K, U]]: ...
    def partitionBy(self, numPartitions: int, partitionFunc: Callable[[K], int] = ...) -> DStream[tuple[K, V]]: ...
    @overload
    def foreachRDD(self, func: Callable[[RDD[T]], None]) -> None: ...
    @overload
    def foreachRDD(self, func: Callable[[datetime, RDD[T]], None]) -> None: ...
    def pprint(self, num: int = 10) -> None: ...
    def mapValues(self, f: Callable[[V], U]) -> DStream[tuple[K, U]]: ...
    def flatMapValues(self, f: Callable[[V], Iterable[U]]) -> DStream[tuple[K, U]]: ...
    def glom(self) -> DStream[list[T]]: ...
    def cache(self) -> DStream[T]: ...
    def persist(self, storageLevel: StorageLevel) -> DStream[T]: ...
    def checkpoint(self, interval: int) -> DStream[T]: ...
    def groupByKey(self, numPartitions: int | None = None) -> DStream[tuple[K, Iterable[V]]]: ...
    def countByValue(self) -> DStream[tuple[K, int]]: ...
    def saveAsTextFiles(self, prefix: str, suffix: str | None = None) -> None: ...
    @overload
    def transform(self, func: Callable[[RDD[T]], RDD[U]]) -> TransformedDStream[U]: ...
    @overload
    def transform(self, func: Callable[[datetime, RDD[T]], RDD[U]]) -> TransformedDStream[U]: ...
    @overload
    def transformWith(self, func: Callable[[RDD[T], RDD[U]], RDD[V]], other: DStream[U], keepSerializer: bool = ...) -> DStream[V]: ...
    @overload
    def transformWith(self, func: Callable[[datetime, RDD[T], RDD[U]], RDD[V]], other: DStream[U], keepSerializer: bool = ...) -> DStream[V]: ...
    def repartition(self, numPartitions: int) -> DStream[T]: ...
    def union(self, other: DStream[U]) -> DStream[T | U]: ...
    def cogroup(self, other: DStream[tuple[K, U]], numPartitions: int | None = None) -> DStream[tuple[K, tuple[ResultIterable[V], ResultIterable[U]]]]: ...
    def join(self, other: DStream[tuple[K, U]], numPartitions: int | None = None) -> DStream[tuple[K, tuple[V, U]]]: ...
    def leftOuterJoin(self, other: DStream[tuple[K, U]], numPartitions: int | None = None) -> DStream[tuple[K, tuple[V, U | None]]]: ...
    def rightOuterJoin(self, other: DStream[tuple[K, U]], numPartitions: int | None = None) -> DStream[tuple[K, tuple[V | None, U]]]: ...
    def fullOuterJoin(self, other: DStream[tuple[K, U]], numPartitions: int | None = None) -> DStream[tuple[K, tuple[V | None, U | None]]]: ...
    def slice(self, begin: datetime | int, end: datetime | int) -> list[RDD[T]]: ...
    def window(self, windowDuration: int, slideDuration: int | None = None) -> DStream[T]: ...
    def reduceByWindow(self, reduceFunc: Callable[[T, T], T], invReduceFunc: Callable[[T, T], T] | None, windowDuration: int, slideDuration: int) -> DStream[T]: ...
    def countByWindow(self, windowDuration: int, slideDuration: int) -> DStream[int]: ...
    def countByValueAndWindow(self, windowDuration: int, slideDuration: int, numPartitions: int | None = None) -> DStream[tuple[T, int]]: ...
    def groupByKeyAndWindow(self, windowDuration: int, slideDuration: int, numPartitions: int | None = None) -> DStream[tuple[K, Iterable[V]]]: ...
    def reduceByKeyAndWindow(self, func: Callable[[V, V], V], invFunc: Callable[[V, V], V] | None, windowDuration: int, slideDuration: int | None = None, numPartitions: int | None = None, filterFunc: Callable[[tuple[K, V]], bool] | None = None) -> DStream[tuple[K, V]]: ...
    def updateStateByKey(self, updateFunc: Callable[[Iterable[V], S | None], S], numPartitions: int | None = None, initialRDD: RDD[tuple[K, S]] | Iterable[tuple[K, S]] | None = None) -> DStream[tuple[K, S]]: ...

class TransformedDStream(DStream[U]):
    @overload
    def __init__(self, prev: DStream[T], func: Callable[[RDD[T]], RDD[U]]) -> None: ...
    @overload
    def __init__(self, prev: DStream[T], func: Callable[[datetime, RDD[T]], RDD[U]]) -> None: ...
