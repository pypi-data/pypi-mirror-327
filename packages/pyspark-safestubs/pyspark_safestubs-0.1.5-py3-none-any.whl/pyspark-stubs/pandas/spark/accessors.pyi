import pyspark.pandas as ps
from abc import ABCMeta, abstractmethod
from pyspark import StorageLevel as StorageLevel
from pyspark._typing import PrimitiveType as PrimitiveType
from pyspark.pandas._typing import IndexOpsLike as IndexOpsLike
from pyspark.pandas.frame import CachedDataFrame as CachedDataFrame
from pyspark.pandas.internal import InternalField as InternalField
from pyspark.sql import Column as PySparkColumn, DataFrame as PySparkDataFrame
from pyspark.sql._typing import OptionalPrimitiveType as OptionalPrimitiveType
from pyspark.sql.types import DataType as DataType, StructType as StructType
from pyspark.sql.utils import get_column_class as get_column_class, get_dataframe_class as get_dataframe_class
from typing import Callable, Generic

class SparkIndexOpsMethods(Generic[IndexOpsLike], metaclass=ABCMeta):
    def __init__(self, data: IndexOpsLike) -> None: ...
    @property
    def data_type(self) -> DataType: ...
    @property
    def nullable(self) -> bool: ...
    @property
    def column(self) -> PySparkColumn: ...
    def transform(self, func: Callable[[PySparkColumn], PySparkColumn]) -> IndexOpsLike: ...
    @property
    @abstractmethod
    def analyzed(self) -> IndexOpsLike: ...

class SparkSeriesMethods(SparkIndexOpsMethods['ps.Series']):
    def apply(self, func: Callable[[PySparkColumn], PySparkColumn]) -> ps.Series: ...
    @property
    def analyzed(self) -> ps.Series: ...

class SparkIndexMethods(SparkIndexOpsMethods['ps.Index']):
    @property
    def analyzed(self) -> ps.Index: ...

class SparkFrameMethods:
    def __init__(self, frame: ps.DataFrame) -> None: ...
    def schema(self, index_col: str | list[str] | None = None) -> StructType: ...
    def print_schema(self, index_col: str | list[str] | None = None) -> None: ...
    def frame(self, index_col: str | list[str] | None = None) -> PySparkDataFrame: ...
    def cache(self) -> CachedDataFrame: ...
    def persist(self, storage_level: StorageLevel = ...) -> CachedDataFrame: ...
    def hint(self, name: str, *parameters: PrimitiveType) -> ps.DataFrame: ...
    def to_table(self, name: str, format: str | None = None, mode: str = 'overwrite', partition_cols: str | list[str] | None = None, index_col: str | list[str] | None = None, **options: OptionalPrimitiveType) -> None: ...
    def to_spark_io(self, path: str | None = None, format: str | None = None, mode: str = 'overwrite', partition_cols: str | list[str] | None = None, index_col: str | list[str] | None = None, **options: OptionalPrimitiveType) -> None: ...
    def explain(self, extended: bool | None = None, mode: str | None = None) -> None: ...
    def apply(self, func: Callable[[PySparkDataFrame], PySparkDataFrame], index_col: str | list[str] | None = None) -> ps.DataFrame: ...
    def repartition(self, num_partitions: int) -> ps.DataFrame: ...
    def coalesce(self, num_partitions: int) -> ps.DataFrame: ...
    def checkpoint(self, eager: bool = True) -> ps.DataFrame: ...
    def local_checkpoint(self, eager: bool = True) -> ps.DataFrame: ...
    @property
    def analyzed(self) -> ps.DataFrame: ...

class CachedSparkFrameMethods(SparkFrameMethods):
    def __init__(self, frame: CachedDataFrame) -> None: ...
    @property
    def storage_level(self) -> StorageLevel: ...
    def unpersist(self) -> None: ...
