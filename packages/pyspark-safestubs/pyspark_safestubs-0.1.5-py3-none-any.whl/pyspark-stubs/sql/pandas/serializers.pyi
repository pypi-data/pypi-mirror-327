from _typeshed import Incomplete
from collections.abc import Generator
from pyspark.errors import PySparkRuntimeError as PySparkRuntimeError, PySparkTypeError as PySparkTypeError, PySparkValueError as PySparkValueError
from pyspark.serializers import CPickleSerializer as CPickleSerializer, Serializer as Serializer, UTF8Deserializer as UTF8Deserializer, read_int as read_int, write_int as write_int
from pyspark.sql.pandas.types import from_arrow_type as from_arrow_type, to_arrow_type as to_arrow_type
from pyspark.sql.types import BinaryType as BinaryType, DataType as DataType, IntegerType as IntegerType, LongType as LongType, StringType as StringType, StructField as StructField, StructType as StructType

class SpecialLengths:
    END_OF_DATA_SECTION: int
    PYTHON_EXCEPTION_THROWN: int
    TIMING_DATA: int
    END_OF_STREAM: int
    NULL: int
    START_ARROW_STREAM: int

class ArrowCollectSerializer(Serializer):
    serializer: Incomplete
    def __init__(self) -> None: ...
    def dump_stream(self, iterator, stream): ...
    def load_stream(self, stream) -> Generator[Incomplete]: ...

class ArrowStreamSerializer(Serializer):
    def dump_stream(self, iterator, stream) -> None: ...
    def load_stream(self, stream) -> Generator[Incomplete]: ...

class ArrowStreamUDFSerializer(ArrowStreamSerializer):
    def load_stream(self, stream) -> Generator[Incomplete]: ...
    def dump_stream(self, iterator, stream): ...

class ArrowStreamPandasSerializer(ArrowStreamSerializer):
    def __init__(self, timezone, safecheck) -> None: ...
    def arrow_to_pandas(self, arrow_column, struct_in_pandas: str = 'dict', ndarray_as_list: bool = False): ...
    def dump_stream(self, iterator, stream) -> None: ...
    def load_stream(self, stream) -> Generator[Incomplete]: ...

class ArrowStreamPandasUDFSerializer(ArrowStreamPandasSerializer):
    def __init__(self, timezone, safecheck, assign_cols_by_name, df_for_struct: bool = False, struct_in_pandas: str = 'dict', ndarray_as_list: bool = False, arrow_cast: bool = False) -> None: ...
    def arrow_to_pandas(self, arrow_column): ...
    def dump_stream(self, iterator, stream): ...

class ArrowStreamPandasUDTFSerializer(ArrowStreamPandasUDFSerializer):
    def __init__(self, timezone, safecheck) -> None: ...

class CogroupUDFSerializer(ArrowStreamPandasUDFSerializer):
    def load_stream(self, stream) -> Generator[Incomplete]: ...

class ApplyInPandasWithStateSerializer(ArrowStreamPandasUDFSerializer):
    pickleSer: Incomplete
    utf8_deserializer: Incomplete
    state_object_schema: Incomplete
    result_count_df_type: Incomplete
    result_count_pdf_arrow_type: Incomplete
    result_state_df_type: Incomplete
    result_state_pdf_arrow_type: Incomplete
    arrow_max_records_per_batch: Incomplete
    def __init__(self, timezone, safecheck, assign_cols_by_name, state_object_schema, arrow_max_records_per_batch) -> None: ...
    def load_stream(self, stream) -> Generator[Incomplete, None, Incomplete]: ...
    def dump_stream(self, iterator, stream): ...
