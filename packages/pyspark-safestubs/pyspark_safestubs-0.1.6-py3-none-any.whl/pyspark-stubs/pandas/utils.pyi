import pandas as pd
from pyspark.errors import PySparkTypeError as PySparkTypeError
from pyspark.pandas._typing import Axis as Axis, DataFrameOrSeries as DataFrameOrSeries, Label as Label, Name as Name
from pyspark.pandas.base import IndexOpsMixin as IndexOpsMixin
from pyspark.pandas.frame import DataFrame as DataFrame
from pyspark.pandas.indexes.base import Index as Index
from pyspark.pandas.internal import InternalFrame as InternalFrame
from pyspark.pandas.series import Series as Series
from pyspark.pandas.typedef.typehints import as_spark_type as as_spark_type
from pyspark.sql import Column as Column, DataFrame as PySparkDataFrame, SparkSession as SparkSession
from pyspark.sql.types import DoubleType as DoubleType
from pyspark.sql.utils import get_dataframe_class as get_dataframe_class, is_remote as is_remote
from typing import Any, Callable, Iterator, overload

ERROR_MESSAGE_CANNOT_COMBINE: str
SPARK_CONF_ARROW_ENABLED: str

class PandasAPIOnSparkAdviceWarning(Warning): ...

def same_anchor(this: DataFrame | IndexOpsMixin | InternalFrame, that: DataFrame | IndexOpsMixin | InternalFrame) -> bool: ...
def combine_frames(this: DataFrame, *args: DataFrameOrSeries, how: str = 'full', preserve_order_column: bool = False) -> DataFrame: ...
def align_diff_frames(resolve_func: Callable[[DataFrame, list[Label], list[Label]], Iterator[tuple['Series', Label]]], this: DataFrame, that: DataFrame, fillna: bool = True, how: str = 'full', preserve_order_column: bool = False) -> DataFrame: ...
def is_testing() -> bool: ...
def default_session() -> SparkSession: ...
def sql_conf(pairs: dict[str, Any], *, spark: SparkSession | None = None) -> Iterator[None]: ...
def validate_arguments_and_invoke_function(pobj: pd.DataFrame | pd.Series, pandas_on_spark_func: Callable, pandas_func: Callable, input_args: dict) -> Any: ...
def lazy_property(fn): ...
def scol_for(sdf: PySparkDataFrame, column_name: str) -> Column: ...
def column_labels_level(column_labels: list[Label]) -> int: ...
def name_like_string(name: Name | None) -> str: ...
def is_name_like_tuple(value: Any, allow_none: bool = True, check_type: bool = False) -> bool: ...
def is_name_like_value(value: Any, allow_none: bool = True, allow_tuple: bool = True, check_type: bool = False) -> bool: ...
def validate_axis(axis: Axis | None = 0, none_axis: int = 0) -> int: ...
def validate_bool_kwarg(value: Any, arg_name: str) -> bool | None: ...
def validate_how(how: str) -> str: ...
def validate_mode(mode: str) -> str: ...
@overload
def verify_temp_column_name(df: PySparkDataFrame, column_name_or_label: str) -> str: ...
@overload
def verify_temp_column_name(df: DataFrame, column_name_or_label: Name) -> Label: ...
def spark_column_equals(left: Column, right: Column) -> bool: ...
def compare_null_first(left: Column, right: Column, comp: Callable[[Column, Column], Column]) -> Column: ...
def compare_null_last(left: Column, right: Column, comp: Callable[[Column, Column], Column]) -> Column: ...
def compare_disallow_null(left: Column, right: Column, comp: Callable[[Column, Column], Column]) -> Column: ...
def compare_allow_null(left: Column, right: Column, comp: Callable[[Column, Column], Column]) -> Column: ...
def log_advice(message: str) -> None: ...
def validate_index_loc(index: Index, loc: int) -> None: ...
