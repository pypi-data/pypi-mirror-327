import pyarrow as pa
import pyspark.sql.connect.proto as proto
from _typeshed import Incomplete
from pyspark.errors import IllegalArgumentException as IllegalArgumentException, PySparkNotImplementedError as PySparkNotImplementedError, PySparkRuntimeError as PySparkRuntimeError, PySparkTypeError as PySparkTypeError
from pyspark.serializers import CloudPickleSerializer as CloudPickleSerializer
from pyspark.sql.connect._typing import ColumnOrName as ColumnOrName
from pyspark.sql.connect.client import SparkConnectClient as SparkConnectClient
from pyspark.sql.connect.column import Column as Column
from pyspark.sql.connect.conversion import storage_level_to_proto as storage_level_to_proto
from pyspark.sql.connect.expressions import ColumnReference as ColumnReference, Expression as Expression, LiteralExpression as LiteralExpression, SortOrder as SortOrder
from pyspark.sql.connect.types import UnparsedDataType as UnparsedDataType, pyspark_types_to_proto_types as pyspark_types_to_proto_types
from pyspark.sql.connect.udf import UserDefinedFunction as UserDefinedFunction
from pyspark.sql.connect.utils import check_dependencies as check_dependencies
from pyspark.sql.types import DataType as DataType
from pyspark.storagelevel import StorageLevel as StorageLevel
from typing import Any, Mapping, Sequence

class LogicalPlan:
    INDENT: int
    def __init__(self, child: LogicalPlan | None) -> None: ...
    def unresolved_attr(self, colName: str) -> proto.Expression: ...
    def to_attr_or_expression(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...
    def to_proto(self, session: SparkConnectClient, debug: bool = False) -> proto.Plan: ...
    def print(self, indent: int = 0) -> str: ...

class DataSource(LogicalPlan):
    def __init__(self, format: str | None = None, schema: str | None = None, options: Mapping[str, str] | None = None, paths: list[str] | None = None, predicates: list[str] | None = None, is_streaming: bool | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Read(LogicalPlan):
    table_name: Incomplete
    options: Incomplete
    def __init__(self, table_name: str, options: dict[str, str] | None = None, is_streaming: bool | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class LocalRelation(LogicalPlan):
    def __init__(self, table: pa.Table | None, schema: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def serialize(self, session: SparkConnectClient) -> bytes: ...
    def print(self, indent: int = 0) -> str: ...

class CachedLocalRelation(LogicalPlan):
    def __init__(self, hash: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class ShowString(LogicalPlan):
    num_rows: Incomplete
    truncate: Incomplete
    vertical: Incomplete
    def __init__(self, child: LogicalPlan | None, num_rows: int, truncate: int, vertical: bool) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class HtmlString(LogicalPlan):
    num_rows: Incomplete
    truncate: Incomplete
    def __init__(self, child: LogicalPlan | None, num_rows: int, truncate: int) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Project(LogicalPlan):
    alias: Incomplete
    def __init__(self, child: LogicalPlan | None, *columns: ColumnOrName) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class WithColumns(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, columnNames: Sequence[str], columns: Sequence[Column], metadata: Sequence[str] | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class WithWatermark(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, event_time: str, delay_threshold: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CachedRemoteRelation(LogicalPlan):
    def __init__(self, relationId: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Hint(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, name: str, parameters: list[Any]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Filter(LogicalPlan):
    filter: Incomplete
    def __init__(self, child: LogicalPlan | None, filter: Column) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Limit(LogicalPlan):
    limit: Incomplete
    def __init__(self, child: LogicalPlan | None, limit: int) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Tail(LogicalPlan):
    limit: Incomplete
    def __init__(self, child: LogicalPlan | None, limit: int) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Offset(LogicalPlan):
    offset: Incomplete
    def __init__(self, child: LogicalPlan | None, offset: int = 0) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Deduplicate(LogicalPlan):
    all_columns_as_keys: Incomplete
    column_names: Incomplete
    within_watermark: Incomplete
    def __init__(self, child: LogicalPlan | None, all_columns_as_keys: bool = False, column_names: list[str] | None = None, within_watermark: bool = False) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Sort(LogicalPlan):
    columns: Incomplete
    is_global: Incomplete
    def __init__(self, child: LogicalPlan | None, columns: list[Column], is_global: bool) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Drop(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, columns: list[Column | str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Sample(LogicalPlan):
    lower_bound: Incomplete
    upper_bound: Incomplete
    with_replacement: Incomplete
    seed: Incomplete
    deterministic_order: Incomplete
    def __init__(self, child: LogicalPlan | None, lower_bound: float, upper_bound: float, with_replacement: bool, seed: int | None, deterministic_order: bool = False) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Aggregate(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, group_type: str, grouping_cols: Sequence[Column], aggregate_cols: Sequence[Column], pivot_col: Column | None, pivot_values: Sequence[Any] | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Join(LogicalPlan):
    left: Incomplete
    right: Incomplete
    on: Incomplete
    how: Incomplete
    def __init__(self, left: LogicalPlan | None, right: LogicalPlan, on: str | list[str] | Column | list[Column] | None, how: str | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class SetOperation(LogicalPlan):
    other: Incomplete
    by_name: Incomplete
    is_all: Incomplete
    set_op: Incomplete
    allow_missing_columns: Incomplete
    def __init__(self, child: LogicalPlan | None, other: LogicalPlan | None, set_op: str, is_all: bool = True, by_name: bool = False, allow_missing_columns: bool = False) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class Repartition(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, num_partitions: int, shuffle: bool) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RepartitionByExpression(LogicalPlan):
    num_partitions: Incomplete
    columns: Incomplete
    def __init__(self, child: LogicalPlan | None, num_partitions: int | None, columns: list['ColumnOrName']) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SubqueryAlias(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, alias: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SQL(LogicalPlan):
    def __init__(self, query: str, args: dict[str, Any] | list | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...

class Range(LogicalPlan):
    def __init__(self, start: int, end: int, step: int, num_partitions: int | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ToSchema(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, schema: DataType) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class WithColumnsRenamed(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, colsMap: Mapping[str, str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Unpivot(LogicalPlan):
    ids: Incomplete
    values: Incomplete
    variable_column_name: Incomplete
    value_column_name: Incomplete
    def __init__(self, child: LogicalPlan | None, ids: list['ColumnOrName'], values: list['ColumnOrName'] | None, variable_column_name: str, value_column_name: str) -> None: ...
    def col_to_expr(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CollectMetrics(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, name: str, exprs: list['ColumnOrName']) -> None: ...
    def col_to_expr(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class NAFill(LogicalPlan):
    cols: Incomplete
    values: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: list[str] | None, values: list[Any]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class NADrop(LogicalPlan):
    cols: Incomplete
    min_non_nulls: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: list[str] | None, min_non_nulls: int | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class NAReplace(LogicalPlan):
    cols: Incomplete
    replacements: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: list[str] | None, replacements: dict[Any, Any]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatSummary(LogicalPlan):
    statistics: Incomplete
    def __init__(self, child: LogicalPlan | None, statistics: list[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatDescribe(LogicalPlan):
    cols: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: list[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatCov(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, col1: str, col2: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatApproxQuantile(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, cols: list[str], probabilities: list[float], relativeError: float) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatCrosstab(LogicalPlan):
    col1: Incomplete
    col2: Incomplete
    def __init__(self, child: LogicalPlan | None, col1: str, col2: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatFreqItems(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, cols: list[str], support: float) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatSampleBy(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, col: ColumnOrName, fractions: dict[Any, float], seed: int | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatCorr(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, col1: str, col2: str, method: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ToDF(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, cols: Sequence[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CreateView(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, name: str, is_global: bool, replace: bool) -> None: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...

class WriteOperation(LogicalPlan):
    source: Incomplete
    path: Incomplete
    table_name: Incomplete
    table_save_method: Incomplete
    mode: Incomplete
    sort_cols: Incomplete
    partitioning_cols: Incomplete
    options: Incomplete
    num_buckets: int
    bucket_cols: Incomplete
    def __init__(self, child: LogicalPlan) -> None: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...
    def print(self, indent: int = 0) -> str: ...

class WriteOperationV2(LogicalPlan):
    table_name: Incomplete
    provider: Incomplete
    partitioning_columns: Incomplete
    options: Incomplete
    table_properties: Incomplete
    mode: Incomplete
    overwrite_condition: Incomplete
    def __init__(self, child: LogicalPlan, table_name: str) -> None: ...
    def col_to_expr(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...

class WriteStreamOperation(LogicalPlan):
    write_op: Incomplete
    def __init__(self, child: LogicalPlan) -> None: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...

class CurrentDatabase(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SetCurrentDatabase(LogicalPlan):
    def __init__(self, db_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListDatabases(LogicalPlan):
    def __init__(self, pattern: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListTables(LogicalPlan):
    def __init__(self, db_name: str | None = None, pattern: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListFunctions(LogicalPlan):
    def __init__(self, db_name: str | None = None, pattern: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListColumns(LogicalPlan):
    def __init__(self, table_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GetDatabase(LogicalPlan):
    def __init__(self, db_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GetTable(LogicalPlan):
    def __init__(self, table_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GetFunction(LogicalPlan):
    def __init__(self, function_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class DatabaseExists(LogicalPlan):
    def __init__(self, db_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class TableExists(LogicalPlan):
    def __init__(self, table_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class FunctionExists(LogicalPlan):
    def __init__(self, function_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CreateExternalTable(LogicalPlan):
    def __init__(self, table_name: str, path: str, source: str | None = None, schema: DataType | None = None, options: Mapping[str, str] = {}) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CreateTable(LogicalPlan):
    def __init__(self, table_name: str, path: str, source: str | None = None, description: str | None = None, schema: DataType | None = None, options: Mapping[str, str] = {}) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class DropTempView(LogicalPlan):
    def __init__(self, view_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class DropGlobalTempView(LogicalPlan):
    def __init__(self, view_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RecoverPartitions(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class IsCached(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CacheTable(LogicalPlan):
    def __init__(self, table_name: str, storage_level: StorageLevel | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class UncacheTable(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ClearCache(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RefreshTable(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RefreshByPath(LogicalPlan):
    def __init__(self, path: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CurrentCatalog(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SetCurrentCatalog(LogicalPlan):
    def __init__(self, catalog_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListCatalogs(LogicalPlan):
    def __init__(self, pattern: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class MapPartitions(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, function: UserDefinedFunction, cols: list[str], is_barrier: bool) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GroupMap(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, grouping_cols: Sequence[Column], function: UserDefinedFunction, cols: list[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CoGroupMap(LogicalPlan):
    def __init__(self, input: LogicalPlan | None, input_grouping_cols: Sequence[Column], other: LogicalPlan | None, other_grouping_cols: Sequence[Column], function: UserDefinedFunction, cols: list[Column]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ApplyInPandasWithState(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, grouping_cols: Sequence[Column], function: UserDefinedFunction, output_schema: str, state_schema: str, output_mode: str, timeout_conf: str, cols: list[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class PythonUDTF:
    def __init__(self, func: type, return_type: DataType | str, eval_type: int, python_ver: str) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.PythonUDTF: ...

class CommonInlineUserDefinedTableFunction(LogicalPlan):
    def __init__(self, function_name: str, function: PythonUDTF, deterministic: bool, arguments: Sequence[Expression]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def udtf_plan(self, session: SparkConnectClient) -> proto.CommonInlineUserDefinedTableFunction: ...

class CachedRelation(LogicalPlan):
    def __init__(self, plan: proto.Relation) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
