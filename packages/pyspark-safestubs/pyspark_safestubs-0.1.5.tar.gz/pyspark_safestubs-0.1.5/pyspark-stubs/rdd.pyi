from _typeshed import Incomplete
from py4j.java_gateway import JavaObject
from pyspark._typing import NonUDFType, NumberOrArray, S
from pyspark.context import SparkContext
from pyspark.resource.profile import ResourceProfile
from pyspark.resultiterable import ResultIterable
from pyspark.serializers import Serializer
from pyspark.sql._typing import SQLArrowBatchedUDFType, SQLArrowTableUDFType, SQLBatchedUDFType, SQLTableUDFType
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.pandas._typing import ArrowMapIterUDFType, PandasCogroupedMapUDFType, PandasGroupedAggUDFType, PandasGroupedMapUDFType, PandasGroupedMapUDFWithStateType, PandasMapIterUDFType, PandasScalarIterUDFType, PandasScalarUDFType, PandasWindowAggUDFType
from pyspark.sql.types import AtomicType, StructType
from pyspark.statcounter import StatCounter
from pyspark.storagelevel import StorageLevel
from typing import Any, Callable, Generic, Hashable, Iterable, Iterator, NoReturn, Sequence, TypeVar, overload

__all__ = ['RDD']

T = TypeVar('T')
T_co = TypeVar('T_co', covariant=True)
U = TypeVar('U')
K = TypeVar('K', bound=Hashable)
V = TypeVar('V')
V1 = TypeVar('V1')
V2 = TypeVar('V2')
V3 = TypeVar('V3')

class PythonEvalType:
    NON_UDF: NonUDFType
    SQL_BATCHED_UDF: SQLBatchedUDFType
    SQL_ARROW_BATCHED_UDF: SQLArrowBatchedUDFType
    SQL_SCALAR_PANDAS_UDF: PandasScalarUDFType
    SQL_GROUPED_MAP_PANDAS_UDF: PandasGroupedMapUDFType
    SQL_GROUPED_AGG_PANDAS_UDF: PandasGroupedAggUDFType
    SQL_WINDOW_AGG_PANDAS_UDF: PandasWindowAggUDFType
    SQL_SCALAR_PANDAS_ITER_UDF: PandasScalarIterUDFType
    SQL_MAP_PANDAS_ITER_UDF: PandasMapIterUDFType
    SQL_COGROUPED_MAP_PANDAS_UDF: PandasCogroupedMapUDFType
    SQL_MAP_ARROW_ITER_UDF: ArrowMapIterUDFType
    SQL_GROUPED_MAP_PANDAS_UDF_WITH_STATE: PandasGroupedMapUDFWithStateType
    SQL_TABLE_UDF: SQLTableUDFType
    SQL_ARROW_TABLE_UDF: SQLArrowTableUDFType

class BoundedFloat(float):
    confidence: float
    low: float
    high: float
    def __new__(cls, mean: float, confidence: float, low: float, high: float) -> BoundedFloat: ...

class Partitioner:
    numPartitions: Incomplete
    partitionFunc: Incomplete
    def __init__(self, numPartitions: int, partitionFunc: Callable[[Any], int]) -> None: ...
    def __eq__(self, other: Any) -> bool: ...
    def __call__(self, k: Any) -> int: ...

class RDD(Generic[T_co]):
    is_cached: bool
    is_checkpointed: bool
    has_resource_profile: bool
    ctx: Incomplete
    partitioner: Incomplete
    def __init__(self, jrdd: JavaObject, ctx: SparkContext, jrdd_deserializer: Serializer = ...) -> None: ...
    def id(self) -> int: ...
    def __getnewargs__(self) -> NoReturn: ...
    @property
    def context(self) -> SparkContext: ...
    def cache(self) -> RDD[T]: ...
    def persist(self, storageLevel: StorageLevel = ...) -> RDD[T]: ...
    def unpersist(self, blocking: bool = False) -> RDD[T]: ...
    def checkpoint(self) -> None: ...
    def isCheckpointed(self) -> bool: ...
    def localCheckpoint(self) -> None: ...
    def isLocallyCheckpointed(self) -> bool: ...
    def getCheckpointFile(self) -> str | None: ...
    def cleanShuffleDependencies(self, blocking: bool = False) -> None: ...
    def map(self, f: Callable[[T], U], preservesPartitioning: bool = False) -> RDD[U]: ...
    def flatMap(self, f: Callable[[T], Iterable[U]], preservesPartitioning: bool = False) -> RDD[U]: ...
    def mapPartitions(self, f: Callable[[Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> RDD[U]: ...
    def mapPartitionsWithIndex(self, f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> RDD[U]: ...
    def mapPartitionsWithSplit(self, f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> RDD[U]: ...
    def getNumPartitions(self) -> int: ...
    def filter(self, f: Callable[[T], bool]) -> RDD[T]: ...
    def distinct(self, numPartitions: int | None = None) -> RDD[T]: ...
    def sample(self, withReplacement: bool, fraction: float, seed: int | None = None) -> RDD[T]: ...
    def randomSplit(self, weights: Sequence[int | float], seed: int | None = None) -> list[RDD[T]]: ...
    def takeSample(self, withReplacement: bool, num: int, seed: int | None = None) -> list[T]: ...
    def union(self, other: RDD[U]) -> RDD[T | U]: ...
    def intersection(self, other: RDD[T]) -> RDD[T]: ...
    def __add__(self, other: RDD[U]) -> RDD[T | U]: ...
    @overload
    def repartitionAndSortWithinPartitions(self, numPartitions: int | None = ..., partitionFunc: Callable[[S], int] = ..., ascending: bool = ...) -> RDD[tuple[S, V]]: ...
    @overload
    def repartitionAndSortWithinPartitions(self, numPartitions: int | None, partitionFunc: Callable[[K], int], ascending: bool, keyfunc: Callable[[K], 'S']) -> RDD[tuple[K, V]]: ...
    @overload
    def repartitionAndSortWithinPartitions(self, numPartitions: int | None = ..., partitionFunc: Callable[[K], int] = ..., ascending: bool = ..., *, keyfunc: Callable[[K], 'S']) -> RDD[tuple[K, V]]: ...
    @overload
    def sortByKey(self, ascending: bool = ..., numPartitions: int | None = ...) -> RDD[tuple[K, V]]: ...
    @overload
    def sortByKey(self, ascending: bool, numPartitions: int, keyfunc: Callable[[K], 'S']) -> RDD[tuple[K, V]]: ...
    @overload
    def sortByKey(self, ascending: bool = ..., numPartitions: int | None = ..., *, keyfunc: Callable[[K], 'S']) -> RDD[tuple[K, V]]: ...
    def sortBy(self, keyfunc: Callable[[T], 'S'], ascending: bool = True, numPartitions: int | None = None) -> RDD[T]: ...
    def glom(self) -> RDD[list[T]]: ...
    def cartesian(self, other: RDD[U]) -> RDD[tuple[T, U]]: ...
    def groupBy(self, f: Callable[[T], K], numPartitions: int | None = None, partitionFunc: Callable[[K], int] = ...) -> RDD[tuple[K, Iterable[T]]]: ...
    def pipe(self, command: str, env: dict[str, str] | None = None, checkCode: bool = False) -> RDD[str]: ...
    def foreach(self, f: Callable[[T], None]) -> None: ...
    def foreachPartition(self, f: Callable[[Iterable[T]], None]) -> None: ...
    def collect(self) -> list[T]: ...
    def collectWithJobGroup(self, groupId: str, description: str, interruptOnCancel: bool = False) -> list[T]: ...
    def reduce(self, f: Callable[[T, T], T]) -> T: ...
    def treeReduce(self, f: Callable[[T, T], T], depth: int = 2) -> T: ...
    def fold(self, zeroValue: T, op: Callable[[T, T], T]) -> T: ...
    def aggregate(self, zeroValue: U, seqOp: Callable[[U, T], U], combOp: Callable[[U, U], U]) -> U: ...
    def treeAggregate(self, zeroValue: U, seqOp: Callable[[U, T], U], combOp: Callable[[U, U], U], depth: int = 2) -> U: ...
    @overload
    def max(self) -> S: ...
    @overload
    def max(self, key: Callable[[T], 'S']) -> T: ...
    @overload
    def min(self) -> S: ...
    @overload
    def min(self, key: Callable[[T], 'S']) -> T: ...
    def sum(self) -> NumberOrArray: ...
    def count(self) -> int: ...
    def stats(self) -> StatCounter: ...
    def histogram(self, buckets: int | list['S'] | tuple['S', ...]) -> tuple[Sequence['S'], list[int]]: ...
    def mean(self) -> float: ...
    def variance(self) -> float: ...
    def stdev(self) -> float: ...
    def sampleStdev(self) -> float: ...
    def sampleVariance(self) -> float: ...
    def countByValue(self) -> dict[K, int]: ...
    @overload
    def top(self, num: int) -> list['S']: ...
    @overload
    def top(self, num: int, key: Callable[[T], 'S']) -> list[T]: ...
    @overload
    def takeOrdered(self, num: int) -> list['S']: ...
    @overload
    def takeOrdered(self, num: int, key: Callable[[T], 'S']) -> list[T]: ...
    def take(self, num: int) -> list[T]: ...
    def first(self) -> T: ...
    def isEmpty(self) -> bool: ...
    def saveAsNewAPIHadoopDataset(self, conf: dict[str, str], keyConverter: str | None = None, valueConverter: str | None = None) -> None: ...
    def saveAsNewAPIHadoopFile(self, path: str, outputFormatClass: str, keyClass: str | None = None, valueClass: str | None = None, keyConverter: str | None = None, valueConverter: str | None = None, conf: dict[str, str] | None = None) -> None: ...
    def saveAsHadoopDataset(self, conf: dict[str, str], keyConverter: str | None = None, valueConverter: str | None = None) -> None: ...
    def saveAsHadoopFile(self, path: str, outputFormatClass: str, keyClass: str | None = None, valueClass: str | None = None, keyConverter: str | None = None, valueConverter: str | None = None, conf: dict[str, str] | None = None, compressionCodecClass: str | None = None) -> None: ...
    def saveAsSequenceFile(self, path: str, compressionCodecClass: str | None = None) -> None: ...
    def saveAsPickleFile(self, path: str, batchSize: int = 10) -> None: ...
    def saveAsTextFile(self, path: str, compressionCodecClass: str | None = None) -> None: ...
    def collectAsMap(self) -> dict[K, V]: ...
    def keys(self) -> RDD[K]: ...
    def values(self) -> RDD[V]: ...
    def reduceByKey(self, func: Callable[[V, V], V], numPartitions: int | None = None, partitionFunc: Callable[[K], int] = ...) -> RDD[tuple[K, V]]: ...
    def reduceByKeyLocally(self, func: Callable[[V, V], V]) -> dict[K, V]: ...
    def countByKey(self) -> dict[K, int]: ...
    def join(self, other: RDD[tuple[K, U]], numPartitions: int | None = None) -> RDD[tuple[K, tuple[V, U]]]: ...
    def leftOuterJoin(self, other: RDD[tuple[K, U]], numPartitions: int | None = None) -> RDD[tuple[K, tuple[V, U | None]]]: ...
    def rightOuterJoin(self, other: RDD[tuple[K, U]], numPartitions: int | None = None) -> RDD[tuple[K, tuple[V | None, U]]]: ...
    def fullOuterJoin(self, other: RDD[tuple[K, U]], numPartitions: int | None = None) -> RDD[tuple[K, tuple[V | None, U | None]]]: ...
    def partitionBy(self, numPartitions: int | None, partitionFunc: Callable[[K], int] = ...) -> RDD[tuple[K, V]]: ...
    def combineByKey(self, createCombiner: Callable[[V], U], mergeValue: Callable[[U, V], U], mergeCombiners: Callable[[U, U], U], numPartitions: int | None = None, partitionFunc: Callable[[K], int] = ...) -> RDD[tuple[K, U]]: ...
    def aggregateByKey(self, zeroValue: U, seqFunc: Callable[[U, V], U], combFunc: Callable[[U, U], U], numPartitions: int | None = None, partitionFunc: Callable[[K], int] = ...) -> RDD[tuple[K, U]]: ...
    def foldByKey(self, zeroValue: V, func: Callable[[V, V], V], numPartitions: int | None = None, partitionFunc: Callable[[K], int] = ...) -> RDD[tuple[K, V]]: ...
    def groupByKey(self, numPartitions: int | None = None, partitionFunc: Callable[[K], int] = ...) -> RDD[tuple[K, Iterable[V]]]: ...
    def flatMapValues(self, f: Callable[[V], Iterable[U]]) -> RDD[tuple[K, U]]: ...
    def mapValues(self, f: Callable[[V], U]) -> RDD[tuple[K, U]]: ...
    @overload
    def groupWith(self, other: RDD[tuple[K, V1]]) -> RDD[tuple[K, tuple[ResultIterable[V], ResultIterable[V1]]]]: ...
    @overload
    def groupWith(self, /, other: RDD[tuple[K, V1]], __o1: RDD[tuple[K, V2]]) -> RDD[tuple[K, tuple[ResultIterable[V], ResultIterable[V1], ResultIterable[V2]]]]: ...
    @overload
    def groupWith(self, other: RDD[tuple[K, V1]], _o1: RDD[tuple[K, V2]], _o2: RDD[tuple[K, V3]]) -> RDD[tuple[K, tuple[ResultIterable[V], ResultIterable[V1], ResultIterable[V2], ResultIterable[V3]]]]: ...
    def cogroup(self, other: RDD[tuple[K, U]], numPartitions: int | None = None) -> RDD[tuple[K, tuple[ResultIterable[V], ResultIterable[U]]]]: ...
    def sampleByKey(self, withReplacement: bool, fractions: dict[K, float | int], seed: int | None = None) -> RDD[tuple[K, V]]: ...
    def subtractByKey(self, other: RDD[tuple[K, Any]], numPartitions: int | None = None) -> RDD[tuple[K, V]]: ...
    def subtract(self, other: RDD[T], numPartitions: int | None = None) -> RDD[T]: ...
    def keyBy(self, f: Callable[[T], K]) -> RDD[tuple[K, T]]: ...
    def repartition(self, numPartitions: int) -> RDD[T]: ...
    def coalesce(self, numPartitions: int, shuffle: bool = False) -> RDD[T]: ...
    def zip(self, other: RDD[U]) -> RDD[tuple[T, U]]: ...
    def zipWithIndex(self) -> RDD[tuple[T, int]]: ...
    def zipWithUniqueId(self) -> RDD[tuple[T, int]]: ...
    def name(self) -> str | None: ...
    def setName(self, name: str) -> RDD[T]: ...
    def toDebugString(self) -> bytes | None: ...
    def getStorageLevel(self) -> StorageLevel: ...
    def lookup(self, key: K) -> list[V]: ...
    def countApprox(self, timeout: int, confidence: float = 0.95) -> int: ...
    def sumApprox(self, timeout: int, confidence: float = 0.95) -> BoundedFloat: ...
    def meanApprox(self, timeout: int, confidence: float = 0.95) -> BoundedFloat: ...
    def countApproxDistinct(self, relativeSD: float = 0.05) -> int: ...
    def toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[T]: ...
    def barrier(self) -> RDDBarrier[T]: ...
    def withResources(self, profile: ResourceProfile) -> RDD[T]: ...
    def getResourceProfile(self) -> ResourceProfile | None: ...
    @overload
    def toDF(self, schema: list[str] | tuple[str, ...] | None = None, sampleRatio: float | None = None) -> DataFrame: ...
    @overload
    def toDF(self, schema: StructType | str | None = None) -> DataFrame: ...
    @overload
    def toDF(self, schema: AtomicType | str) -> DataFrame: ...

class RDDBarrier(Generic[T]):
    rdd: Incomplete
    def __init__(self, rdd: RDD[T]) -> None: ...
    def mapPartitions(self, f: Callable[[Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> RDD[U]: ...
    def mapPartitionsWithIndex(self, f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> RDD[U]: ...

class PipelinedRDD(RDD[U], Generic[T, U]):
    func: Incomplete
    preservesPartitioning: Incomplete
    is_cached: bool
    has_resource_profile: bool
    is_checkpointed: bool
    ctx: Incomplete
    prev: Incomplete
    partitioner: Incomplete
    is_barrier: Incomplete
    def __init__(self, prev: RDD[T], func: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool = False, isFromBarrier: bool = False) -> None: ...
    def getNumPartitions(self) -> int: ...
    def id(self) -> int: ...
