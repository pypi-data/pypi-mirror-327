from _typeshed import Incomplete
from pyspark import shuffle as shuffle
from pyspark.broadcast import Broadcast as Broadcast
from pyspark.errors import PySparkRuntimeError as PySparkRuntimeError, PySparkTypeError as PySparkTypeError
from pyspark.files import SparkFiles as SparkFiles
from pyspark.java_gateway import local_connect_and_auth as local_connect_and_auth
from pyspark.rdd import PythonEvalType as PythonEvalType
from pyspark.resource import ResourceInformation as ResourceInformation
from pyspark.serializers import BatchedSerializer as BatchedSerializer, CPickleSerializer as CPickleSerializer, SpecialLengths as SpecialLengths, UTF8Deserializer as UTF8Deserializer, read_bool as read_bool, read_int as read_int, read_long as read_long, write_int as write_int, write_long as write_long, write_with_length as write_with_length
from pyspark.sql.pandas.serializers import ApplyInPandasWithStateSerializer as ApplyInPandasWithStateSerializer, ArrowStreamPandasUDFSerializer as ArrowStreamPandasUDFSerializer, ArrowStreamPandasUDTFSerializer as ArrowStreamPandasUDTFSerializer, ArrowStreamUDFSerializer as ArrowStreamUDFSerializer, CogroupUDFSerializer as CogroupUDFSerializer
from pyspark.sql.pandas.types import to_arrow_type as to_arrow_type
from pyspark.sql.types import BinaryType as BinaryType, StringType as StringType, StructType as StructType
from pyspark.taskcontext import BarrierTaskContext as BarrierTaskContext, TaskContext as TaskContext
from pyspark.util import fail_on_stopiteration as fail_on_stopiteration, try_simplify_traceback as try_simplify_traceback

has_resource_module: bool
pickleSer: Incomplete
utf8_deserializer: Incomplete

def report_times(outfile, boot, init, finish) -> None: ...
def add_path(path) -> None: ...
def read_command(serializer, file): ...
def chain(f, g): ...
def wrap_udf(f, return_type): ...
def wrap_scalar_pandas_udf(f, return_type): ...
def wrap_arrow_batch_udf(f, return_type): ...
def wrap_pandas_batch_iter_udf(f, return_type): ...
def verify_pandas_result(result, return_type, assign_cols_by_name, truncate_return_schema) -> None: ...
def wrap_arrow_batch_iter_udf(f, return_type): ...
def wrap_cogrouped_map_pandas_udf(f, return_type, argspec, runner_conf): ...
def wrap_grouped_map_pandas_udf(f, return_type, argspec, runner_conf): ...
def wrap_grouped_map_pandas_udf_with_state(f, return_type): ...
def wrap_grouped_agg_pandas_udf(f, return_type): ...
def wrap_window_agg_pandas_udf(f, return_type, runner_conf, udf_index): ...
def wrap_unbounded_window_agg_pandas_udf(f, return_type): ...
def wrap_bounded_window_agg_pandas_udf(f, return_type): ...
def read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index): ...
def assign_cols_by_name(runner_conf): ...
def read_udtf(pickleSer, infile, eval_type): ...
def read_udfs(pickleSer, infile, eval_type): ...
def main(infile, outfile) -> None: ...
