from pyspark.sql.types import Row, StructType

__all__ = ['GroupState', 'GroupStateTimeout']

class GroupStateTimeout:
    NoTimeout: str
    ProcessingTimeTimeout: str
    EventTimeTimeout: str

class GroupState:
    NO_TIMESTAMP: int
    def __init__(self, optionalValue: Row, batchProcessingTimeMs: int, eventTimeWatermarkMs: int, timeoutConf: str, hasTimedOut: bool, watermarkPresent: bool, defined: bool, updated: bool, removed: bool, timeoutTimestamp: int, keyAsUnsafe: bytes, valueSchema: StructType) -> None: ...
    @property
    def exists(self) -> bool: ...
    @property
    def get(self) -> tuple: ...
    @property
    def getOption(self) -> tuple | None: ...
    @property
    def hasTimedOut(self) -> bool: ...
    @property
    def oldTimeoutTimestamp(self) -> int: ...
    def update(self, newValue: tuple) -> None: ...
    def remove(self) -> None: ...
    def setTimeoutDuration(self, durationMs: int) -> None: ...
    def setTimeoutTimestamp(self, timestampMs: int) -> None: ...
    def getCurrentWatermarkMs(self) -> int: ...
    def getCurrentProcessingTimeMs(self) -> int: ...
    def json(self) -> str: ...
