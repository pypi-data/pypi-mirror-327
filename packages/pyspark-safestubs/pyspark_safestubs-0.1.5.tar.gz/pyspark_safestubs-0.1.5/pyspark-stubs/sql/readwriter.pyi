from pyspark import RDD
from pyspark.sql._typing import OptionalPrimitiveType
from pyspark.sql.column import Column
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.session import SparkSession
from pyspark.sql.types import StructType
from typing import overload

__all__ = ['DataFrameReader', 'DataFrameWriter', 'DataFrameWriterV2']

PathOrPaths = str | list[str]
TupleOrListOfString = list[str] | tuple[str, ...]

class OptionUtils: ...

class DataFrameReader(OptionUtils):
    def __init__(self, spark: SparkSession) -> None: ...
    def format(self, source: str) -> DataFrameReader: ...
    def schema(self, schema: StructType | str) -> DataFrameReader: ...
    def option(self, key: str, value: OptionalPrimitiveType) -> DataFrameReader: ...
    def options(self, **options: OptionalPrimitiveType) -> DataFrameReader: ...
    def load(self, path: PathOrPaths | None = None, format: str | None = None, schema: StructType | str | None = None, **options: OptionalPrimitiveType) -> DataFrame: ...
    def json(self, path: str | list[str] | RDD[str], schema: StructType | str | None = None, primitivesAsString: bool | str | None = None, prefersDecimal: bool | str | None = None, allowComments: bool | str | None = None, allowUnquotedFieldNames: bool | str | None = None, allowSingleQuotes: bool | str | None = None, allowNumericLeadingZero: bool | str | None = None, allowBackslashEscapingAnyCharacter: bool | str | None = None, mode: str | None = None, columnNameOfCorruptRecord: str | None = None, dateFormat: str | None = None, timestampFormat: str | None = None, multiLine: bool | str | None = None, allowUnquotedControlChars: bool | str | None = None, lineSep: str | None = None, samplingRatio: float | str | None = None, dropFieldIfAllNull: bool | str | None = None, encoding: str | None = None, locale: str | None = None, pathGlobFilter: bool | str | None = None, recursiveFileLookup: bool | str | None = None, modifiedBefore: bool | str | None = None, modifiedAfter: bool | str | None = None, allowNonNumericNumbers: bool | str | None = None) -> DataFrame: ...
    def table(self, tableName: str) -> DataFrame: ...
    def parquet(self, *paths: str, **options: OptionalPrimitiveType) -> DataFrame: ...
    def text(self, paths: PathOrPaths, wholetext: bool = False, lineSep: str | None = None, pathGlobFilter: bool | str | None = None, recursiveFileLookup: bool | str | None = None, modifiedBefore: bool | str | None = None, modifiedAfter: bool | str | None = None) -> DataFrame: ...
    def csv(self, path: PathOrPaths, schema: StructType | str | None = None, sep: str | None = None, encoding: str | None = None, quote: str | None = None, escape: str | None = None, comment: str | None = None, header: bool | str | None = None, inferSchema: bool | str | None = None, ignoreLeadingWhiteSpace: bool | str | None = None, ignoreTrailingWhiteSpace: bool | str | None = None, nullValue: str | None = None, nanValue: str | None = None, positiveInf: str | None = None, negativeInf: str | None = None, dateFormat: str | None = None, timestampFormat: str | None = None, maxColumns: int | str | None = None, maxCharsPerColumn: int | str | None = None, maxMalformedLogPerPartition: int | str | None = None, mode: str | None = None, columnNameOfCorruptRecord: str | None = None, multiLine: bool | str | None = None, charToEscapeQuoteEscaping: str | None = None, samplingRatio: float | str | None = None, enforceSchema: bool | str | None = None, emptyValue: str | None = None, locale: str | None = None, lineSep: str | None = None, pathGlobFilter: bool | str | None = None, recursiveFileLookup: bool | str | None = None, modifiedBefore: bool | str | None = None, modifiedAfter: bool | str | None = None, unescapedQuoteHandling: str | None = None) -> DataFrame: ...
    def orc(self, path: PathOrPaths, mergeSchema: bool | None = None, pathGlobFilter: bool | str | None = None, recursiveFileLookup: bool | str | None = None, modifiedBefore: bool | str | None = None, modifiedAfter: bool | str | None = None) -> DataFrame: ...
    @overload
    def jdbc(self, url: str, table: str, *, properties: dict[str, str] | None = None) -> DataFrame: ...
    @overload
    def jdbc(self, url: str, table: str, column: str, lowerBound: int | str, upperBound: int | str, numPartitions: int, *, properties: dict[str, str] | None = None) -> DataFrame: ...
    @overload
    def jdbc(self, url: str, table: str, *, predicates: list[str], properties: dict[str, str] | None = None) -> DataFrame: ...

class DataFrameWriter(OptionUtils):
    def __init__(self, df: DataFrame) -> None: ...
    def mode(self, saveMode: str | None) -> DataFrameWriter: ...
    def format(self, source: str) -> DataFrameWriter: ...
    def option(self, key: str, value: OptionalPrimitiveType) -> DataFrameWriter: ...
    def options(self, **options: OptionalPrimitiveType) -> DataFrameWriter: ...
    @overload
    def partitionBy(self, *cols: str) -> DataFrameWriter: ...
    @overload
    def partitionBy(self, *cols: list[str]) -> DataFrameWriter: ...
    @overload
    def bucketBy(self, numBuckets: int, col: str, *cols: str) -> DataFrameWriter: ...
    @overload
    def bucketBy(self, numBuckets: int, col: TupleOrListOfString) -> DataFrameWriter: ...
    @overload
    def sortBy(self, col: str, *cols: str) -> DataFrameWriter: ...
    @overload
    def sortBy(self, col: TupleOrListOfString) -> DataFrameWriter: ...
    def save(self, path: str | None = None, format: str | None = None, mode: str | None = None, partitionBy: str | list[str] | None = None, **options: OptionalPrimitiveType) -> None: ...
    def insertInto(self, tableName: str, overwrite: bool | None = None) -> None: ...
    def saveAsTable(self, name: str, format: str | None = None, mode: str | None = None, partitionBy: str | list[str] | None = None, **options: OptionalPrimitiveType) -> None: ...
    def json(self, path: str, mode: str | None = None, compression: str | None = None, dateFormat: str | None = None, timestampFormat: str | None = None, lineSep: str | None = None, encoding: str | None = None, ignoreNullFields: bool | str | None = None) -> None: ...
    def parquet(self, path: str, mode: str | None = None, partitionBy: str | list[str] | None = None, compression: str | None = None) -> None: ...
    def text(self, path: str, compression: str | None = None, lineSep: str | None = None) -> None: ...
    def csv(self, path: str, mode: str | None = None, compression: str | None = None, sep: str | None = None, quote: str | None = None, escape: str | None = None, header: bool | str | None = None, nullValue: str | None = None, escapeQuotes: bool | str | None = None, quoteAll: bool | str | None = None, dateFormat: str | None = None, timestampFormat: str | None = None, ignoreLeadingWhiteSpace: bool | str | None = None, ignoreTrailingWhiteSpace: bool | str | None = None, charToEscapeQuoteEscaping: str | None = None, encoding: str | None = None, emptyValue: str | None = None, lineSep: str | None = None) -> None: ...
    def orc(self, path: str, mode: str | None = None, partitionBy: str | list[str] | None = None, compression: str | None = None) -> None: ...
    def jdbc(self, url: str, table: str, mode: str | None = None, properties: dict[str, str] | None = None) -> None: ...

class DataFrameWriterV2:
    def __init__(self, df: DataFrame, table: str) -> None: ...
    def using(self, provider: str) -> DataFrameWriterV2: ...
    def option(self, key: str, value: OptionalPrimitiveType) -> DataFrameWriterV2: ...
    def options(self, **options: OptionalPrimitiveType) -> DataFrameWriterV2: ...
    def tableProperty(self, property: str, value: str) -> DataFrameWriterV2: ...
    def partitionedBy(self, col: Column, *cols: Column) -> DataFrameWriterV2: ...
    def create(self) -> None: ...
    def replace(self) -> None: ...
    def createOrReplace(self) -> None: ...
    def append(self) -> None: ...
    def overwrite(self, condition: Column) -> None: ...
    def overwritePartitions(self) -> None: ...
