from _typeshed import Incomplete
from py4j.java_gateway import JavaGateway as JavaGateway, JavaObject as JavaObject
from pyspark import SparkContext as SparkContext, since as since
from pyspark.ml._typing import PipelineStage as PipelineStage
from pyspark.ml.base import Params as Params
from pyspark.ml.common import inherit_doc as inherit_doc
from pyspark.ml.wrapper import JavaWrapper as JavaWrapper
from pyspark.sql import SparkSession as SparkSession
from pyspark.sql.utils import is_remote as is_remote
from pyspark.util import VersionUtils as VersionUtils
from typing import Any, Callable, Generic, Sequence, TypeVar

T = TypeVar('T')
RW = TypeVar('RW', bound='BaseReadWrite')
W = TypeVar('W', bound='MLWriter')
JW = TypeVar('JW', bound='JavaMLWriter')
RL = TypeVar('RL', bound='MLReadable')
JR = TypeVar('JR', bound='JavaMLReader')
FuncT = TypeVar('FuncT', bound=Callable[..., Any])

class Identifiable:
    uid: Incomplete
    def __init__(self) -> None: ...

class BaseReadWrite:
    def __init__(self) -> None: ...
    def session(self, sparkSession: SparkSession) -> RW: ...
    @property
    def sparkSession(self) -> SparkSession: ...
    @property
    def sc(self) -> SparkContext: ...

class MLWriter(BaseReadWrite):
    shouldOverwrite: bool
    optionMap: Incomplete
    def __init__(self) -> None: ...
    def save(self, path: str) -> None: ...
    def saveImpl(self, path: str) -> None: ...
    def overwrite(self) -> MLWriter: ...
    def option(self, key: str, value: Any) -> MLWriter: ...

class GeneralMLWriter(MLWriter):
    source: Incomplete
    def format(self, source: str) -> GeneralMLWriter: ...

class JavaMLWriter(MLWriter):
    def __init__(self, instance: JavaMLWritable) -> None: ...
    def save(self, path: str) -> None: ...
    def overwrite(self) -> JavaMLWriter: ...
    def option(self, key: str, value: str) -> JavaMLWriter: ...
    def session(self, sparkSession: SparkSession) -> JavaMLWriter: ...

class GeneralJavaMLWriter(JavaMLWriter):
    def __init__(self, instance: JavaMLWritable) -> None: ...
    def format(self, source: str) -> GeneralJavaMLWriter: ...

class MLWritable:
    def write(self) -> MLWriter: ...
    def save(self, path: str) -> None: ...

class JavaMLWritable(MLWritable):
    def write(self) -> JavaMLWriter: ...

class GeneralJavaMLWritable(JavaMLWritable):
    def write(self) -> GeneralJavaMLWriter: ...

class MLReader(BaseReadWrite, Generic[RL]):
    def __init__(self) -> None: ...
    def load(self, path: str) -> RL: ...

class JavaMLReader(MLReader[RL]):
    def __init__(self, clazz: type['JavaMLReadable[RL]']) -> None: ...
    def load(self, path: str) -> RL: ...
    def session(self, sparkSession: SparkSession) -> JR: ...

class MLReadable(Generic[RL]):
    @classmethod
    def read(cls) -> MLReader[RL]: ...
    @classmethod
    def load(cls, path: str) -> RL: ...

class JavaMLReadable(MLReadable[RL]):
    @classmethod
    def read(cls) -> JavaMLReader[RL]: ...

class DefaultParamsWritable(MLWritable):
    def write(self) -> MLWriter: ...

class DefaultParamsWriter(MLWriter):
    instance: Incomplete
    def __init__(self, instance: Params) -> None: ...
    def saveImpl(self, path: str) -> None: ...
    @staticmethod
    def extractJsonParams(instance: Params, skipParams: Sequence[str]) -> dict[str, Any]: ...
    @staticmethod
    def saveMetadata(instance: Params, path: str, sc: SparkContext, extraMetadata: dict[str, Any] | None = None, paramMap: dict[str, Any] | None = None) -> None: ...

class DefaultParamsReadable(MLReadable[RL]):
    @classmethod
    def read(cls) -> DefaultParamsReader[RL]: ...

class DefaultParamsReader(MLReader[RL]):
    cls: Incomplete
    def __init__(self, cls: type[DefaultParamsReadable[RL]]) -> None: ...
    def load(self, path: str) -> RL: ...
    @staticmethod
    def loadMetadata(path: str, sc: SparkContext, expectedClassName: str = '') -> dict[str, Any]: ...
    @staticmethod
    def getAndSetParams(instance: RL, metadata: dict[str, Any], skipParams: list[str] | None = None) -> None: ...
    @staticmethod
    def isPythonParamsInstance(metadata: dict[str, Any]) -> bool: ...
    @staticmethod
    def loadParamsInstance(path: str, sc: SparkContext) -> RL: ...

class HasTrainingSummary(Generic[T]):
    @property
    def hasSummary(self) -> bool: ...
    @property
    def summary(self) -> T: ...

class MetaAlgorithmReadWrite:
    @staticmethod
    def isMetaEstimator(pyInstance: Any) -> bool: ...
    @staticmethod
    def getAllNestedStages(pyInstance: Any) -> list['Params']: ...
    @staticmethod
    def getUidMap(instance: Any) -> dict[str, 'Params']: ...

def try_remote_functions(f: FuncT) -> FuncT: ...
