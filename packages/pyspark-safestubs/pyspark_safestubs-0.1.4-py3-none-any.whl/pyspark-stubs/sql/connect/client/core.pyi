import grpc
import pandas as pd
import pyarrow as pa
import pyspark.sql.connect.proto as pb2
from _typeshed import Incomplete
from pyspark.sql.connect._typing import DataTypeOrString
from pyspark.sql.types import DataType, StructType
from pyspark.storagelevel import StorageLevel
from types import TracebackType
from typing import Any, Callable, Generator, Iterable, Iterator

__all__ = ['ChannelBuilder', 'SparkConnectClient', 'getLogLevel']

def getLogLevel() -> int | None: ...

class ChannelBuilder:
    PARAM_USE_SSL: str
    PARAM_TOKEN: str
    PARAM_USER_ID: str
    PARAM_USER_AGENT: str
    PARAM_SESSION_ID: str
    MAX_MESSAGE_LENGTH: Incomplete
    @staticmethod
    def default_port() -> int: ...
    url: Incomplete
    params: Incomplete
    def __init__(self, url: str, channelOptions: list[tuple[str, Any]] | None = None) -> None: ...
    def metadata(self) -> Iterable[tuple[str, str]]: ...
    @property
    def secure(self) -> bool: ...
    @property
    def endpoint(self) -> str: ...
    @property
    def userId(self) -> str | None: ...
    @property
    def userAgent(self) -> str: ...
    def get(self, key: str) -> Any: ...
    @property
    def session_id(self) -> str | None: ...
    def toChannel(self) -> grpc.Channel: ...

class MetricValue:
    def __init__(self, name: str, value: int | float, type: str) -> None: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int | float: ...
    @property
    def metric_type(self) -> str: ...

class PlanMetrics:
    def __init__(self, name: str, id: int, parent: int, metrics: list[MetricValue]) -> None: ...
    @property
    def name(self) -> str: ...
    @property
    def plan_id(self) -> int: ...
    @property
    def parent_plan_id(self) -> int: ...
    @property
    def metrics(self) -> list[MetricValue]: ...

class PlanObservedMetrics:
    def __init__(self, name: str, metrics: list[pb2.Expression.Literal]) -> None: ...
    @property
    def name(self) -> str: ...
    @property
    def metrics(self) -> list[pb2.Expression.Literal]: ...

class AnalyzeResult:
    schema: Incomplete
    explain_string: Incomplete
    tree_string: Incomplete
    is_local: Incomplete
    is_streaming: Incomplete
    input_files: Incomplete
    spark_version: Incomplete
    parsed: Incomplete
    is_same_semantics: Incomplete
    semantic_hash: Incomplete
    storage_level: Incomplete
    def __init__(self, schema: DataType | None, explain_string: str | None, tree_string: str | None, is_local: bool | None, is_streaming: bool | None, input_files: list[str] | None, spark_version: str | None, parsed: DataType | None, is_same_semantics: bool | None, semantic_hash: int | None, storage_level: StorageLevel | None) -> None: ...
    @classmethod
    def fromProto(cls, pb: Any) -> AnalyzeResult: ...

class ConfigResult:
    pairs: Incomplete
    warnings: Incomplete
    def __init__(self, pairs: list[tuple[str, str | None]], warnings: list[str]) -> None: ...
    @classmethod
    def fromProto(cls, pb: pb2.ConfigResponse) -> ConfigResult: ...

class SparkConnectClient:
    @classmethod
    def retry_exception(cls, e: Exception) -> bool: ...
    thread_local: Incomplete
    def __init__(self, connection: str | ChannelBuilder, user_id: str | None = None, channel_options: list[tuple[str, Any]] | None = None, retry_policy: dict[str, Any] | None = None, use_reattachable_execute: bool = True) -> None: ...
    def disable_reattachable_execute(self) -> SparkConnectClient: ...
    def enable_reattachable_execute(self) -> SparkConnectClient: ...
    def register_udf(self, function: Any, return_type: DataTypeOrString, name: str | None = None, eval_type: int = ..., deterministic: bool = True) -> str: ...
    def register_udtf(self, function: Any, return_type: DataTypeOrString, name: str, eval_type: int = ..., deterministic: bool = True) -> str: ...
    def register_java(self, name: str, javaClassName: str, return_type: DataTypeOrString | None = None, aggregate: bool = False) -> None: ...
    def to_table_as_iterator(self, plan: pb2.Plan) -> Iterator[StructType | pa.Table]: ...
    def to_table(self, plan: pb2.Plan) -> tuple['pa.Table', StructType | None]: ...
    def to_pandas(self, plan: pb2.Plan) -> pd.DataFrame: ...
    def schema(self, plan: pb2.Plan) -> StructType: ...
    def explain_string(self, plan: pb2.Plan, explain_mode: str = 'extended') -> str: ...
    def execute_command(self, command: pb2.Command) -> tuple[pd.DataFrame | None, dict[str, Any]]: ...
    def same_semantics(self, plan: pb2.Plan, other: pb2.Plan) -> bool: ...
    def semantic_hash(self, plan: pb2.Plan) -> int: ...
    def close(self) -> None: ...
    @property
    def is_closed(self) -> bool: ...
    @property
    def host(self) -> str: ...
    @property
    def token(self) -> str | None: ...
    def get_configs(self, *keys: str) -> tuple[str | None, ...]: ...
    def get_config_with_defaults(self, *pairs: tuple[str, str | None]) -> tuple[str | None, ...]: ...
    def config(self, operation: pb2.ConfigRequest.Operation) -> ConfigResult: ...
    def interrupt_all(self) -> list[str] | None: ...
    def interrupt_tag(self, tag: str) -> list[str] | None: ...
    def interrupt_operation(self, op_id: str) -> list[str] | None: ...
    def add_tag(self, tag: str) -> None: ...
    def remove_tag(self, tag: str) -> None: ...
    def get_tags(self) -> set[str]: ...
    def clear_tags(self) -> None: ...
    def add_artifacts(self, *path: str, pyfile: bool, archive: bool, file: bool) -> None: ...
    def copy_from_local_to_fs(self, local_path: str, dest_path: str) -> None: ...
    def cache_artifact(self, blob: bytes) -> str: ...

class RetryState:
    def __init__(self) -> None: ...
    def set_exception(self, exc: BaseException) -> None: ...
    def throw(self) -> None: ...
    def set_done(self) -> None: ...
    def count(self) -> int: ...
    def done(self) -> bool: ...

class AttemptManager:
    def __init__(self, check: Callable[..., bool], retry_state: RetryState) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None) -> bool | None: ...
    def is_first_try(self) -> bool: ...

class Retrying:
    def __init__(self, max_retries: int, initial_backoff: int, max_backoff: int, backoff_multiplier: float, jitter: int, min_jitter_threshold: int, can_retry: Callable[..., bool] = ..., sleep: Callable[[float], None] = ...) -> None: ...
    def __iter__(self) -> Generator[AttemptManager, None, None]: ...
