import numpy as np
import pandas as pd
from pyspark import SparkConf as SparkConf, SparkContext as SparkContext, __version__ as __version__
from pyspark.errors import PySparkAttributeError as PySparkAttributeError, PySparkNotImplementedError as PySparkNotImplementedError, PySparkRuntimeError as PySparkRuntimeError, PySparkTypeError as PySparkTypeError, PySparkValueError as PySparkValueError
from pyspark.sql.connect._typing import OptionalPrimitiveType as OptionalPrimitiveType
from pyspark.sql.connect.catalog import Catalog as Catalog
from pyspark.sql.connect.client import ChannelBuilder as ChannelBuilder, SparkConnectClient as SparkConnectClient
from pyspark.sql.connect.conf import RuntimeConf as RuntimeConf
from pyspark.sql.connect.dataframe import DataFrame as DataFrame
from pyspark.sql.connect.plan import CachedLocalRelation as CachedLocalRelation, CachedRelation as CachedRelation, CachedRemoteRelation as CachedRemoteRelation, LocalRelation as LocalRelation, LogicalPlan as LogicalPlan, Range as Range, SQL as SQL
from pyspark.sql.connect.readwriter import DataFrameReader as DataFrameReader
from pyspark.sql.connect.streaming import DataStreamReader as DataStreamReader, StreamingQueryManager as StreamingQueryManager
from pyspark.sql.connect.udf import UDFRegistration as UDFRegistration
from pyspark.sql.connect.udtf import UDTFRegistration as UDTFRegistration
from pyspark.sql.connect.utils import check_dependencies as check_dependencies
from pyspark.sql.pandas.serializers import ArrowStreamPandasSerializer as ArrowStreamPandasSerializer
from pyspark.sql.pandas.types import to_arrow_schema as to_arrow_schema, to_arrow_type as to_arrow_type
from pyspark.sql.session import classproperty as classproperty
from pyspark.sql.types import AtomicType as AtomicType, DataType as DataType, DayTimeIntervalType as DayTimeIntervalType, Row as Row, StructType as StructType, TimestampType as TimestampType
from pyspark.sql.utils import to_str as to_str
from typing import Any, Iterable, overload

class SparkSession:
    class Builder:
        def __init__(self) -> None: ...
        @overload
        def config(self, key: str, value: Any) -> SparkSession.Builder: ...
        @overload
        def config(self, *, map: dict[str, 'OptionalPrimitiveType']) -> SparkSession.Builder: ...
        def master(self, master: str) -> SparkSession.Builder: ...
        def appName(self, name: str) -> SparkSession.Builder: ...
        def remote(self, location: str = 'sc://localhost') -> SparkSession.Builder: ...
        def channelBuilder(self, channelBuilder: ChannelBuilder) -> SparkSession.Builder: ...
        def enableHiveSupport(self) -> SparkSession.Builder: ...
        def create(self) -> SparkSession: ...
        def getOrCreate(self) -> SparkSession: ...
    def builder(cls) -> Builder: ...
    def __init__(self, connection: str | ChannelBuilder, userId: str | None = None) -> None: ...
    @classmethod
    def getActiveSession(cls) -> SparkSession | None: ...
    @classmethod
    def active(cls) -> SparkSession: ...
    def table(self, tableName: str) -> DataFrame: ...
    @property
    def read(self) -> DataFrameReader: ...
    @property
    def readStream(self) -> DataStreamReader: ...
    def createDataFrame(self, data: pd.DataFrame | np.ndarray | Iterable[Any], schema: AtomicType | StructType | str | list[str] | tuple[str, ...] | None = None) -> DataFrame: ...
    def sql(self, sqlQuery: str, args: dict[str, Any] | list | None = None) -> DataFrame: ...
    def range(self, start: int, end: int | None = None, step: int = 1, numPartitions: int | None = None) -> DataFrame: ...
    @property
    def catalog(self) -> Catalog: ...
    def __del__(self) -> None: ...
    def interruptAll(self) -> list[str]: ...
    def interruptTag(self, tag: str) -> list[str]: ...
    def interruptOperation(self, op_id: str) -> list[str]: ...
    def addTag(self, tag: str) -> None: ...
    def removeTag(self, tag: str) -> None: ...
    def getTags(self) -> set[str]: ...
    def clearTags(self) -> None: ...
    def stop(self) -> None: ...
    @property
    def is_stopped(self) -> bool: ...
    @property
    def conf(self) -> RuntimeConf: ...
    @property
    def streams(self) -> StreamingQueryManager: ...
    def __getattr__(self, name: str) -> Any: ...
    @property
    def udf(self) -> UDFRegistration: ...
    @property
    def udtf(self) -> UDTFRegistration: ...
    @property
    def version(self) -> str: ...
    @property
    def client(self) -> SparkConnectClient: ...
    def addArtifacts(self, *path: str, pyfile: bool = False, archive: bool = False, file: bool = False) -> None: ...
    addArtifact = addArtifacts
    def copyFromLocalToFs(self, local_path: str, dest_path: str) -> None: ...
    @property
    def session_id(self) -> str: ...
