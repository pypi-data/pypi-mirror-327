from _typeshed import Incomplete
from py4j.java_collections import JavaMap
from pyspark.context import SparkContext
from pyspark.mllib._typing import VectorLike
from pyspark.mllib.common import JavaModelWrapper
from pyspark.mllib.linalg import Vector
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.util import JavaLoader, JavaSaveable
from pyspark.rdd import RDD
from typing import Hashable, Iterable, overload

__all__ = ['Normalizer', 'StandardScalerModel', 'StandardScaler', 'HashingTF', 'IDFModel', 'IDF', 'Word2Vec', 'Word2VecModel', 'ChiSqSelector', 'ChiSqSelectorModel', 'ElementwiseProduct']

class VectorTransformer:
    @overload
    def transform(self, vector: VectorLike) -> Vector: ...
    @overload
    def transform(self, vector: RDD['VectorLike']) -> RDD[Vector]: ...

class Normalizer(VectorTransformer):
    p: Incomplete
    def __init__(self, p: float = 2.0) -> None: ...
    @overload
    def transform(self, vector: VectorLike) -> Vector: ...
    @overload
    def transform(self, vector: RDD['VectorLike']) -> RDD[Vector]: ...

class JavaVectorTransformer(JavaModelWrapper, VectorTransformer):
    @overload
    def transform(self, vector: VectorLike) -> Vector: ...
    @overload
    def transform(self, vector: RDD['VectorLike']) -> RDD[Vector]: ...

class StandardScalerModel(JavaVectorTransformer):
    @overload
    def transform(self, vector: VectorLike) -> Vector: ...
    @overload
    def transform(self, vector: RDD['VectorLike']) -> RDD[Vector]: ...
    def setWithMean(self, withMean: bool) -> StandardScalerModel: ...
    def setWithStd(self, withStd: bool) -> StandardScalerModel: ...
    @property
    def withStd(self) -> bool: ...
    @property
    def withMean(self) -> bool: ...
    @property
    def std(self) -> Vector: ...
    @property
    def mean(self) -> Vector: ...

class StandardScaler:
    withMean: Incomplete
    withStd: Incomplete
    def __init__(self, withMean: bool = False, withStd: bool = True) -> None: ...
    def fit(self, dataset: RDD['VectorLike']) -> StandardScalerModel: ...

class ChiSqSelectorModel(JavaVectorTransformer):
    @overload
    def transform(self, vector: VectorLike) -> Vector: ...
    @overload
    def transform(self, vector: RDD['VectorLike']) -> RDD[Vector]: ...

class ChiSqSelector:
    numTopFeatures: Incomplete
    selectorType: Incomplete
    percentile: Incomplete
    fpr: Incomplete
    fdr: Incomplete
    fwe: Incomplete
    def __init__(self, numTopFeatures: int = 50, selectorType: str = 'numTopFeatures', percentile: float = 0.1, fpr: float = 0.05, fdr: float = 0.05, fwe: float = 0.05) -> None: ...
    def setNumTopFeatures(self, numTopFeatures: int) -> ChiSqSelector: ...
    def setPercentile(self, percentile: float) -> ChiSqSelector: ...
    def setFpr(self, fpr: float) -> ChiSqSelector: ...
    def setFdr(self, fdr: float) -> ChiSqSelector: ...
    def setFwe(self, fwe: float) -> ChiSqSelector: ...
    def setSelectorType(self, selectorType: str) -> ChiSqSelector: ...
    def fit(self, data: RDD[LabeledPoint]) -> ChiSqSelectorModel: ...

class PCAModel(JavaVectorTransformer): ...

class PCA:
    k: Incomplete
    def __init__(self, k: int) -> None: ...
    def fit(self, data: RDD['VectorLike']) -> PCAModel: ...

class HashingTF:
    numFeatures: Incomplete
    binary: bool
    def __init__(self, numFeatures: int = ...) -> None: ...
    def setBinary(self, value: bool) -> HashingTF: ...
    def indexOf(self, term: Hashable) -> int: ...
    @overload
    def transform(self, document: Iterable[Hashable]) -> Vector: ...
    @overload
    def transform(self, document: RDD[Iterable[Hashable]]) -> RDD[Vector]: ...

class IDFModel(JavaVectorTransformer):
    @overload
    def transform(self, x: VectorLike) -> Vector: ...
    @overload
    def transform(self, x: RDD['VectorLike']) -> RDD[Vector]: ...
    def idf(self) -> Vector: ...
    def docFreq(self) -> list[int]: ...
    def numDocs(self) -> int: ...

class IDF:
    minDocFreq: Incomplete
    def __init__(self, minDocFreq: int = 0) -> None: ...
    def fit(self, dataset: RDD['VectorLike']) -> IDFModel: ...

class Word2VecModel(JavaVectorTransformer, JavaSaveable, JavaLoader['Word2VecModel']):
    def transform(self, word: str) -> Vector: ...
    def findSynonyms(self, word: str | VectorLike, num: int) -> Iterable[tuple[str, float]]: ...
    def getVectors(self) -> JavaMap: ...
    @classmethod
    def load(cls, sc: SparkContext, path: str) -> Word2VecModel: ...

class Word2Vec:
    vectorSize: int
    learningRate: float
    numPartitions: int
    numIterations: int
    seed: Incomplete
    minCount: int
    windowSize: int
    def __init__(self) -> None: ...
    def setVectorSize(self, vectorSize: int) -> Word2Vec: ...
    def setLearningRate(self, learningRate: float) -> Word2Vec: ...
    def setNumPartitions(self, numPartitions: int) -> Word2Vec: ...
    def setNumIterations(self, numIterations: int) -> Word2Vec: ...
    def setSeed(self, seed: int) -> Word2Vec: ...
    def setMinCount(self, minCount: int) -> Word2Vec: ...
    def setWindowSize(self, windowSize: int) -> Word2Vec: ...
    def fit(self, data: RDD[list[str]]) -> Word2VecModel: ...

class ElementwiseProduct(VectorTransformer):
    scalingVector: Incomplete
    def __init__(self, scalingVector: Vector) -> None: ...
    @overload
    def transform(self, vector: VectorLike) -> Vector: ...
    @overload
    def transform(self, vector: RDD['VectorLike']) -> RDD[Vector]: ...
