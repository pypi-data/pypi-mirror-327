import pandas
import pyspark.sql.connect.plan as plan
from _typeshed import Incomplete
from pyspark._globals import _NoValueType
from pyspark.errors import PySparkAttributeError as PySparkAttributeError, PySparkNotImplementedError as PySparkNotImplementedError, PySparkTypeError as PySparkTypeError, PySparkValueError as PySparkValueError
from pyspark.errors.exceptions.base import SessionNotSameException as SessionNotSameException
from pyspark.errors.exceptions.connect import SparkConnectException as SparkConnectException
from pyspark.pandas.frame import DataFrame as PandasOnSparkDataFrame
from pyspark.rdd import PythonEvalType as PythonEvalType
from pyspark.sql.connect._typing import ArrowMapIterFunction as ArrowMapIterFunction, ColumnOrName as ColumnOrName, LiteralType as LiteralType, OptionalPrimitiveType as OptionalPrimitiveType, PandasMapIterFunction as PandasMapIterFunction, PrimitiveType as PrimitiveType
from pyspark.sql.connect.column import Column as Column
from pyspark.sql.connect.expressions import UnresolvedRegex as UnresolvedRegex
from pyspark.sql.connect.functions import col as col, lit as lit
from pyspark.sql.connect.group import GroupedData as GroupedData
from pyspark.sql.connect.readwriter import DataFrameWriter as DataFrameWriter, DataFrameWriterV2 as DataFrameWriterV2
from pyspark.sql.connect.session import SparkSession as SparkSession
from pyspark.sql.connect.streaming.readwriter import DataStreamWriter as DataStreamWriter
from pyspark.sql.connect.utils import check_dependencies as check_dependencies
from pyspark.sql.observation import Observation as Observation
from pyspark.sql.pandas.types import from_arrow_schema as from_arrow_schema
from pyspark.sql.types import Row as Row, StructType as StructType
from pyspark.storagelevel import StorageLevel as StorageLevel
from typing import Any, Callable, Iterator, overload

class DataFrame:
    def __init__(self, session: SparkSession, schema: StructType | None = None) -> None: ...
    @property
    def write(self) -> DataFrameWriter: ...
    def isEmpty(self) -> bool: ...
    def select(self, *cols: ColumnOrName) -> DataFrame: ...
    def selectExpr(self, *expr: str | list[str]) -> DataFrame: ...
    def agg(self, *exprs: Column | dict[str, str]) -> DataFrame: ...
    def alias(self, alias: str) -> DataFrame: ...
    def colRegex(self, colName: str) -> Column: ...
    @property
    def dtypes(self) -> list[tuple[str, str]]: ...
    @property
    def columns(self) -> list[str]: ...
    @property
    def sparkSession(self) -> SparkSession: ...
    def count(self) -> int: ...
    def crossJoin(self, other: DataFrame) -> DataFrame: ...
    def checkSameSparkSession(self, other: DataFrame) -> None: ...
    def coalesce(self, numPartitions: int) -> DataFrame: ...
    @overload
    def repartition(self, numPartitions: int, *cols: ColumnOrName) -> DataFrame: ...
    @overload
    def repartition(self, *cols: ColumnOrName) -> DataFrame: ...
    @overload
    def repartitionByRange(self, numPartitions: int, *cols: ColumnOrName) -> DataFrame: ...
    @overload
    def repartitionByRange(self, *cols: ColumnOrName) -> DataFrame: ...
    def dropDuplicates(self, subset: list[str] | None = None) -> DataFrame: ...
    drop_duplicates = dropDuplicates
    def dropDuplicatesWithinWatermark(self, subset: list[str] | None = None) -> DataFrame: ...
    drop_duplicates_within_watermark = dropDuplicatesWithinWatermark
    def distinct(self) -> DataFrame: ...
    def drop(self, *cols: ColumnOrName) -> DataFrame: ...
    def filter(self, condition: Column | str) -> DataFrame: ...
    def first(self) -> Row | None: ...
    def groupBy(self, *cols: ColumnOrName) -> GroupedData: ...
    groupby = groupBy
    def rollup(self, *cols: ColumnOrName) -> GroupedData: ...
    def cube(self, *cols: ColumnOrName) -> GroupedData: ...
    @overload
    def head(self) -> Row | None: ...
    @overload
    def head(self, n: int) -> list[Row]: ...
    def take(self, num: int) -> list[Row]: ...
    def join(self, other: DataFrame, on: str | list[str] | Column | list[Column] | None = None, how: str | None = None) -> DataFrame: ...
    def limit(self, n: int) -> DataFrame: ...
    def tail(self, num: int) -> list[Row]: ...
    def sort(self, *cols: str | Column | list[str | Column], **kwargs: Any) -> DataFrame: ...
    orderBy = sort
    def sortWithinPartitions(self, *cols: str | Column | list[str | Column], **kwargs: Any) -> DataFrame: ...
    def sample(self, withReplacement: float | bool | None = None, fraction: int | float | None = None, seed: int | None = None) -> DataFrame: ...
    def withColumnRenamed(self, existing: str, new: str) -> DataFrame: ...
    def withColumnsRenamed(self, colsMap: dict[str, str]) -> DataFrame: ...
    def withColumns(self, colsMap: dict[str, Column]) -> DataFrame: ...
    def withColumn(self, colName: str, col: Column) -> DataFrame: ...
    def withMetadata(self, columnName: str, metadata: dict[str, Any]) -> DataFrame: ...
    def unpivot(self, ids: ColumnOrName | list['ColumnOrName'] | tuple['ColumnOrName', ...], values: ColumnOrName | list['ColumnOrName'] | tuple['ColumnOrName', ...] | None, variableColumnName: str, valueColumnName: str) -> DataFrame: ...
    melt = unpivot
    def withWatermark(self, eventTime: str, delayThreshold: str) -> DataFrame: ...
    def hint(self, name: str, *parameters: PrimitiveType | list['PrimitiveType']) -> DataFrame: ...
    def randomSplit(self, weights: list[float], seed: int | None = None) -> list['DataFrame']: ...
    def observe(self, observation: Observation | str, *exprs: Column) -> DataFrame: ...
    def show(self, n: int = 20, truncate: bool | int = True, vertical: bool = False) -> None: ...
    def union(self, other: DataFrame) -> DataFrame: ...
    def unionAll(self, other: DataFrame) -> DataFrame: ...
    def unionByName(self, other: DataFrame, allowMissingColumns: bool = False) -> DataFrame: ...
    def subtract(self, other: DataFrame) -> DataFrame: ...
    def exceptAll(self, other: DataFrame) -> DataFrame: ...
    def intersect(self, other: DataFrame) -> DataFrame: ...
    def intersectAll(self, other: DataFrame) -> DataFrame: ...
    def where(self, condition: Column | str) -> DataFrame: ...
    @property
    def na(self) -> DataFrameNaFunctions: ...
    def fillna(self, value: LiteralType | dict[str, 'LiteralType'], subset: str | tuple[str, ...] | list[str] | None = None) -> DataFrame: ...
    def dropna(self, how: str = 'any', thresh: int | None = None, subset: str | tuple[str, ...] | list[str] | None = None) -> DataFrame: ...
    def replace(self, to_replace: LiteralType | list['LiteralType'] | dict['LiteralType', 'OptionalPrimitiveType'], value: OptionalPrimitiveType | list['OptionalPrimitiveType'] | _NoValueType | None = ..., subset: list[str] | None = None) -> DataFrame: ...
    @property
    def stat(self) -> DataFrameStatFunctions: ...
    def summary(self, *statistics: str) -> DataFrame: ...
    def describe(self, *cols: str | list[str]) -> DataFrame: ...
    def cov(self, col1: str, col2: str) -> float: ...
    def corr(self, col1: str, col2: str, method: str | None = None) -> float: ...
    def approxQuantile(self, col: str | list[str] | tuple[str], probabilities: list[float] | tuple[float], relativeError: float) -> list[float] | list[list[float]]: ...
    def crosstab(self, col1: str, col2: str) -> DataFrame: ...
    def freqItems(self, cols: list[str] | tuple[str], support: float | None = None) -> DataFrame: ...
    def sampleBy(self, col: ColumnOrName, fractions: dict[Any, float], seed: int | None = None) -> DataFrame: ...
    def __getattr__(self, name: str) -> Column: ...
    @overload
    def __getitem__(self, item: int | str) -> Column: ...
    @overload
    def __getitem__(self, item: Column | list | tuple) -> DataFrame: ...
    def __dir__(self) -> list[str]: ...
    def collect(self) -> list[Row]: ...
    def toPandas(self) -> pandas.DataFrame: ...
    @property
    def schema(self) -> StructType: ...
    def isLocal(self) -> bool: ...
    @property
    def isStreaming(self) -> bool: ...
    def printSchema(self, level: int | None = None) -> None: ...
    def inputFiles(self) -> list[str]: ...
    def to(self, schema: StructType) -> DataFrame: ...
    def toDF(self, *cols: str) -> DataFrame: ...
    def transform(self, func: Callable[..., 'DataFrame'], *args: Any, **kwargs: Any) -> DataFrame: ...
    def explain(self, extended: bool | str | None = None, mode: str | None = None) -> None: ...
    def createTempView(self, name: str) -> None: ...
    def createOrReplaceTempView(self, name: str) -> None: ...
    def createGlobalTempView(self, name: str) -> None: ...
    def createOrReplaceGlobalTempView(self, name: str) -> None: ...
    def cache(self) -> DataFrame: ...
    def persist(self, storageLevel: StorageLevel = ...) -> DataFrame: ...
    @property
    def storageLevel(self) -> StorageLevel: ...
    def unpersist(self, blocking: bool = False) -> DataFrame: ...
    @property
    def is_cached(self) -> bool: ...
    def toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[Row]: ...
    def to_pandas_on_spark(self, index_col: str | list[str] | None = None) -> PandasOnSparkDataFrame: ...
    def pandas_api(self, index_col: str | list[str] | None = None) -> PandasOnSparkDataFrame: ...
    def registerTempTable(self, name: str) -> None: ...
    def mapInPandas(self, func: PandasMapIterFunction, schema: StructType | str, barrier: bool = False) -> DataFrame: ...
    def mapInArrow(self, func: ArrowMapIterFunction, schema: StructType | str, barrier: bool = False) -> DataFrame: ...
    @property
    def writeStream(self) -> DataStreamWriter: ...
    def sameSemantics(self, other: DataFrame) -> bool: ...
    def semanticHash(self) -> int: ...
    def writeTo(self, table: str) -> DataFrameWriterV2: ...
    def offset(self, n: int) -> DataFrame: ...
    @classmethod
    def withPlan(cls, plan: plan.LogicalPlan, session: SparkSession) -> DataFrame: ...

class DataFrameNaFunctions:
    df: Incomplete
    def __init__(self, df: DataFrame) -> None: ...
    def fill(self, value: LiteralType | dict[str, 'LiteralType'], subset: str | tuple[str, ...] | list[str] | None = None) -> DataFrame: ...
    def drop(self, how: str = 'any', thresh: int | None = None, subset: str | tuple[str, ...] | list[str] | None = None) -> DataFrame: ...
    def replace(self, to_replace: list['LiteralType'] | dict['LiteralType', 'OptionalPrimitiveType'], value: OptionalPrimitiveType | list['OptionalPrimitiveType'] | _NoValueType | None = ..., subset: list[str] | None = None) -> DataFrame: ...

class DataFrameStatFunctions:
    df: Incomplete
    def __init__(self, df: DataFrame) -> None: ...
    def cov(self, col1: str, col2: str) -> float: ...
    def corr(self, col1: str, col2: str, method: str | None = None) -> float: ...
    def approxQuantile(self, col: str | list[str] | tuple[str], probabilities: list[float] | tuple[float], relativeError: float) -> list[float] | list[list[float]]: ...
    def crosstab(self, col1: str, col2: str) -> DataFrame: ...
    def freqItems(self, cols: list[str] | tuple[str], support: float | None = None) -> DataFrame: ...
    def sampleBy(self, col: str, fractions: dict[Any, float], seed: int | None = None) -> DataFrame: ...
